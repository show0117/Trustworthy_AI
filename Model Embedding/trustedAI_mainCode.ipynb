{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trustedAI_mainCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMeIFWkWtCSs"
      },
      "source": [
        "## Fashion Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZfvWg7bPIBd",
        "outputId": "66a440ca-85f3-48e9-c508-01d0d74bd98c"
      },
      "source": [
        "# Google Drive\n",
        "!gdown --id '1OqmmCIz1uwbj5RNkjpN_9Fnk6rdjJRES' --output fashion.zip\n",
        "\n",
        "# Unzip the dataset.\n",
        "# This may take some time.\n",
        "!unzip -q fashion.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OqmmCIz1uwbj5RNkjpN_9Fnk6rdjJRES\n",
            "To: /content/fashion.zip\n",
            "72.1MB [00:01, 67.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVrKci4PUFW"
      },
      "source": [
        "# Import necessary packages.\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as model\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.datasets import DatasetFolder\n",
        "import struct\n",
        "\n",
        "import pandas as pd\n",
        "import torch.hub\n",
        "from typing import Any\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0i9ZCPrOVN_"
      },
      "source": [
        "## 原始模型表現\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc-yLh6ng1qP"
      },
      "source": [
        "train_csv = pd.read_csv(\"fashion-mnist_train.csv\")\n",
        "test_csv = pd.read_csv(\"fashion-mnist_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "UhmzzQQ7uSUl",
        "outputId": "00b3ed2f-0cb9-4e5a-ae76-8d0439daf95e"
      },
      "source": [
        "train_csv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>92</td>\n",
              "      <td>101</td>\n",
              "      <td>107</td>\n",
              "      <td>100</td>\n",
              "      <td>132</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>...</td>\n",
              "      <td>211</td>\n",
              "      <td>220</td>\n",
              "      <td>214</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>222</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>134</td>\n",
              "      <td>162</td>\n",
              "      <td>191</td>\n",
              "      <td>214</td>\n",
              "      <td>163</td>\n",
              "      <td>146</td>\n",
              "      <td>165</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>114</td>\n",
              "      <td>183</td>\n",
              "      <td>112</td>\n",
              "      <td>55</td>\n",
              "      <td>23</td>\n",
              "      <td>72</td>\n",
              "      <td>102</td>\n",
              "      <td>165</td>\n",
              "      <td>160</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>188</td>\n",
              "      <td>163</td>\n",
              "      <td>93</td>\n",
              "      <td>...</td>\n",
              "      <td>171</td>\n",
              "      <td>249</td>\n",
              "      <td>207</td>\n",
              "      <td>197</td>\n",
              "      <td>202</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>21</td>\n",
              "      <td>25</td>\n",
              "      <td>69</td>\n",
              "      <td>52</td>\n",
              "      <td>45</td>\n",
              "      <td>74</td>\n",
              "      <td>39</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>187</td>\n",
              "      <td>189</td>\n",
              "      <td>...</td>\n",
              "      <td>230</td>\n",
              "      <td>237</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>116</td>\n",
              "      <td>112</td>\n",
              "      <td>136</td>\n",
              "      <td>147</td>\n",
              "      <td>144</td>\n",
              "      <td>121</td>\n",
              "      <td>102</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  ...  pixel781  pixel782  pixel783  pixel784\n",
              "0      2       0       0       0  ...         0         0         0         0\n",
              "1      9       0       0       0  ...         0         0         0         0\n",
              "2      6       0       0       0  ...         0         0         0         0\n",
              "3      0       0       0       0  ...         0         0         0         0\n",
              "4      3       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TH2px8GhJus"
      },
      "source": [
        "class FashionDataset(Dataset):\n",
        "  def __init__(self, data, transform = None):\n",
        "    \"\"\"Method to initilaize variables.\"\"\" \n",
        "    self.fashion_MNIST = list(data.values)\n",
        "    self.transform = transform\n",
        "    \n",
        "    label = []\n",
        "    image = []\n",
        "    \n",
        "    for i in self.fashion_MNIST:\n",
        "      label.append(i[0])\n",
        "      image.append(i[1:])\n",
        "    self.labels = np.asarray(label)\n",
        "    print(self.labels)\n",
        "    self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label = self.labels[index]\n",
        "    image = self.images[index]\n",
        "    \n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Hx7oswhj3i",
        "outputId": "1646fd44-16c6-4e8e-ebfb-d69ca3ab0b91"
      },
      "source": [
        "batch_size = 100\n",
        "train_set = FashionDataset(train_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
        "test_loader = DataLoader(train_set, batch_size=batch_size, num_workers=8, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 9 6 ... 8 8 7]\n",
            "[0 1 2 ... 8 8 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMWG9ZhKh9s6"
      },
      "source": [
        "def output_label(label):\n",
        "  output_mapping = {\n",
        "    0: \"T-shirt/Top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\", \n",
        "    5: \"Sandal\", \n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\"\n",
        "    }\n",
        "  input = (label.item() if type(label) == torch.Tensor else label)\n",
        "  return output_mapping[input]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o47q6WJLbk7A"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(AlexNet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(1, 32, kernel_size=3, padding=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((3, 3))\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(256 * 3 * 3, 1024),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(1024, 512),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(512, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swlf5EwA-hxA",
        "outputId": "ddfc7b45-b19d-41c8-9196-d25c708541fd"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AlexNet(num_classes=10).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (x,y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if batch%100==0:\n",
        "    loss, current = loss.item(), batch * len(x)\n",
        "    print(f\"loss: {loss: >7f}[{curret: >5d}/{size: >5d}]\")\n",
        "\n",
        "def _test(dataloader, model):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      pred = model(x)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  print(f\"Test error: \\n Acc: {correct}, Avg Loss:{test_loss}\")\n",
        "\n",
        "epoch = 10\n",
        "for t in range(epoch):\n",
        "  print(f\"Epoch {t+1}\\n-----------------\")\n",
        "  train(train_loader, model, loss_fn, optimizer)\n",
        "  _test(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test error: \n",
            " Acc: 0.7327333333333333, Avg Loss:0.0074508494784434635\n",
            "Epoch 2\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8069666666666667, Avg Loss:0.005352747803926468\n",
            "Epoch 3\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.83765, Avg Loss:0.004619357679784298\n",
            "Epoch 4\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8541, Avg Loss:0.0041768316278855\n",
            "Epoch 5\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8636666666666667, Avg Loss:0.003865952519575755\n",
            "Epoch 6\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8709666666666667, Avg Loss:0.0036320103374620277\n",
            "Epoch 7\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.87735, Avg Loss:0.0034474132421116036\n",
            "Epoch 8\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8822666666666666, Avg Loss:0.0033034680505593618\n",
            "Epoch 9\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.88635, Avg Loss:0.0031773423805832863\n",
            "Epoch 10\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8904166666666666, Avg Loss:0.0030648924012978873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrktgHPr6Ten"
      },
      "source": [
        "## 取得目標malware執行檔轉浮點數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWT8tQSLxPAk"
      },
      "source": [
        "import struct\n",
        "with open('/content/Virus.vbs',mode = 'r') as f:\n",
        "    file = f.read()\n",
        "\n",
        "def str_to_byte(x):\n",
        "    num = ord(x)\n",
        "    byte = chr(num)\n",
        "    return byte\n",
        "\n",
        "bytes_ = []\n",
        "for i in file:\n",
        "    byte = str_to_byte(i)\n",
        "    bytes_ = bytes_ + [byte]\n",
        "\n",
        "bytes_list = []  \n",
        "prefix = ['\\x3c']\n",
        "for i in range(0,270):\n",
        "    comb = prefix+bytes_[i*3:(i+1)*3]\n",
        "    bytes_list = bytes_list + [comb]\n",
        "\n",
        "new_list = []\n",
        "for i in bytes_list:\n",
        "    word = ''\n",
        "    for j in i:\n",
        "        word = word + j\n",
        "    bword = bytes(word, 'utf-8')\n",
        "    param = struct.unpack(\">f\", bword)\n",
        "    new_list = new_list + [param[0]]\n",
        "malFloat = new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4UUqzu9CPRU"
      },
      "source": [
        "malFloat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtUpFwP11N7F"
      },
      "source": [
        "### 尋找可替換的神經元"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixm8Kb416X4j",
        "outputId": "9444e900-3e27-41d7-90cf-3a50392412ee"
      },
      "source": [
        "model_ft = model\n",
        "model_ft.parameters()\n",
        "model_dict = model_ft.state_dict()\n",
        "model_dict.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.8.weight', 'features.8.bias', 'features.10.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFXO70tb6L44",
        "outputId": "24b07f53-d378-4bd7-f477-97d33154f784"
      },
      "source": [
        "fullyConn = []\n",
        "for i in model_dict.keys():\n",
        "    if 'classifier' in i and 'weight' in i:\n",
        "        fullyConn.append(i)\n",
        "fullyConn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['classifier.1.weight', 'classifier.4.weight', 'classifier.6.weight']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhHfIh9XBO44",
        "outputId": "fb9d9595-922d-4094-e7ba-44f434c4f8ce"
      },
      "source": [
        "decode_conv = []\n",
        "decode_j = []\n",
        "decode_k = []\n",
        "codes = []\n",
        "for param in malFloat:\n",
        "  print(param)\n",
        "  b = ''\n",
        "  for i in fullyConn:\n",
        "    if b == 'break':\n",
        "      break\n",
        "    for j in range(len(model_dict[i])):\n",
        "      if b == 'break':\n",
        "        break\n",
        "      for k in range(len(model_dict[i][j])):\n",
        "        neuron = model_dict[i][j][k]\n",
        "        code = i+str(j)+str(k)\n",
        "        if code not in codes:\n",
        "          if abs(neuron.cpu().numpy().tolist()-param)<=0.0001:\n",
        "            decode_conv = decode_conv + [i]\n",
        "            decode_j = decode_j + [j]\n",
        "            decode_k = decode_k + [k]\n",
        "            codes = codes + [code]\n",
        "            b = 'break'\n",
        "        if b == 'break':\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.00989747978746891\n",
            "0.01319820899516344\n",
            "0.014856161549687386\n",
            "0.014616049826145172\n",
            "0.009909017942845821\n",
            "0.014980822801589966\n",
            "0.013881546445190907\n",
            "0.014676440507173538\n",
            "0.014004260301589966\n",
            "0.014248639345169067\n",
            "0.014247512444853783\n",
            "0.01400076225399971\n",
            "0.014000923372805119\n",
            "0.009834559634327888\n",
            "0.010509531013667583\n",
            "0.010816138237714767\n",
            "0.009907820262014866\n",
            "0.014918421395123005\n",
            "0.009897500276565552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obZaBS5VC7zG"
      },
      "source": [
        "# 將可替換神經元座標存成.csv檔案\n",
        "Infected_Neurons = pd.DataFrame({\n",
        "    'layer':decode_conv,\n",
        "    'param1':decode_j,\n",
        "    'param2':decode_k,\n",
        "})\n",
        "Infected_Neurons.to_csv('Infected_Neurons.csv',index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p00gaMs7bke"
      },
      "source": [
        "## 開始替換神經元"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F4IT2pmEpQX"
      },
      "source": [
        "new_model = model_ft.state_dict().copy()\n",
        "for i in range(len(Infected_Neurons)):\n",
        "  layer = Infected_Neurons['layer'][i]\n",
        "  param1 = Infected_Neurons['param1'][i]\n",
        "  param2 = Infected_Neurons['param2'][i]\n",
        "  new_param = torch.Tensor([malFloat[i]])[0]\n",
        "  new_model[layer][param1][param2] = new_param\n",
        "torch.save(new_model,'Infected_Model.pt') # 受損的模型權重檔案"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdGtKXPktCS7"
      },
      "source": [
        "### 測試替換後模型表現"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW26ANlM75JW",
        "outputId": "0866929f-d1e6-4428-f173-41e905e56576"
      },
      "source": [
        "model_ft.load_state_dict(new_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZtckUhc70il",
        "outputId": "48add941-41d4-4774-b39d-9d14990ca726"
      },
      "source": [
        "_test(test_loader, model_ft)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test error: \n",
            " Acc: 0.8904166666666666, Avg Loss:0.0030649043987194696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL5UFKYW79Bt"
      },
      "source": [
        "## 結論\n",
        "### 我們成功替換了所有受損神經元，且模型預測能力完全沒有下降。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBKQeYWC-KUM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTq_bhk2-KWR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BpZvaWO-KYd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}