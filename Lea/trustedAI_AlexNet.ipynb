{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trustedAI_AlexNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhzdomRTOKoJ",
        "outputId": "db3f14d0-b501-45d9-9206-6cf995ffbfc7"
      },
      "source": [
        "# Google Drive\n",
        "!gdown --id '1OqmmCIz1uwbj5RNkjpN_9Fnk6rdjJRES' --output fashion.zip\n",
        "\n",
        "# Unzip the dataset.\n",
        "# This may take some time.\n",
        "!unzip -q fashion.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OqmmCIz1uwbj5RNkjpN_9Fnk6rdjJRES\n",
            "To: /content/fashion.zip\n",
            "72.1MB [00:01, 70.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVrKci4PUFW"
      },
      "source": [
        "# Import necessary packages.\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as model\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
        "from torch.utils.data import Dataset, ConcatDataset, DataLoader, Subset\n",
        "from torchvision.datasets import DatasetFolder\n",
        "\n",
        "import pandas as pd\n",
        "import torch.hub\n",
        "from typing import Any\n",
        "\n",
        "# This is for the progress bar.\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0i9ZCPrOVN_"
      },
      "source": [
        "## 原始模型表現\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc-yLh6ng1qP"
      },
      "source": [
        "\n",
        "train_csv = pd.read_csv(\"fashion-mnist_train.csv\")\n",
        "test_csv = pd.read_csv(\"fashion-mnist_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "UhmzzQQ7uSUl",
        "outputId": "1de9943a-cd69-4eee-9a21-8d662568f2d6"
      },
      "source": [
        "train_csv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>92</td>\n",
              "      <td>101</td>\n",
              "      <td>107</td>\n",
              "      <td>100</td>\n",
              "      <td>132</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>...</td>\n",
              "      <td>211</td>\n",
              "      <td>220</td>\n",
              "      <td>214</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>222</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>134</td>\n",
              "      <td>162</td>\n",
              "      <td>191</td>\n",
              "      <td>214</td>\n",
              "      <td>163</td>\n",
              "      <td>146</td>\n",
              "      <td>165</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>114</td>\n",
              "      <td>183</td>\n",
              "      <td>112</td>\n",
              "      <td>55</td>\n",
              "      <td>23</td>\n",
              "      <td>72</td>\n",
              "      <td>102</td>\n",
              "      <td>165</td>\n",
              "      <td>160</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>188</td>\n",
              "      <td>163</td>\n",
              "      <td>93</td>\n",
              "      <td>...</td>\n",
              "      <td>171</td>\n",
              "      <td>249</td>\n",
              "      <td>207</td>\n",
              "      <td>197</td>\n",
              "      <td>202</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>21</td>\n",
              "      <td>25</td>\n",
              "      <td>69</td>\n",
              "      <td>52</td>\n",
              "      <td>45</td>\n",
              "      <td>74</td>\n",
              "      <td>39</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>187</td>\n",
              "      <td>189</td>\n",
              "      <td>...</td>\n",
              "      <td>230</td>\n",
              "      <td>237</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>116</td>\n",
              "      <td>112</td>\n",
              "      <td>136</td>\n",
              "      <td>147</td>\n",
              "      <td>144</td>\n",
              "      <td>121</td>\n",
              "      <td>102</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  ...  pixel781  pixel782  pixel783  pixel784\n",
              "0      2       0       0       0  ...         0         0         0         0\n",
              "1      9       0       0       0  ...         0         0         0         0\n",
              "2      6       0       0       0  ...         0         0         0         0\n",
              "3      0       0       0       0  ...         0         0         0         0\n",
              "4      3       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TH2px8GhJus"
      },
      "source": [
        "class FashionDataset(Dataset):\n",
        "  def __init__(self, data, transform = None):\n",
        "    \"\"\"Method to initilaize variables.\"\"\" \n",
        "    self.fashion_MNIST = list(data.values)\n",
        "    self.transform = transform\n",
        "    \n",
        "    label = []\n",
        "    image = []\n",
        "    \n",
        "    for i in self.fashion_MNIST:\n",
        "      label.append(i[0])\n",
        "      image.append(i[1:])\n",
        "    self.labels = np.asarray(label)\n",
        "    print(self.labels)\n",
        "    self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label = self.labels[index]\n",
        "    image = self.images[index]\n",
        "    \n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Hx7oswhj3i",
        "outputId": "c3e02cf4-8828-47d6-91a4-530863eb704a"
      },
      "source": [
        "batch_size = 100\n",
        "train_set = FashionDataset(train_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
        "test_loader = DataLoader(train_set, batch_size=batch_size, num_workers=8, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 9 6 ... 8 8 7]\n",
            "[0 1 2 ... 8 8 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMWG9ZhKh9s6"
      },
      "source": [
        "def output_label(label):\n",
        "  output_mapping = {\n",
        "    0: \"T-shirt/Top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\", \n",
        "    5: \"Sandal\", \n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\"\n",
        "    }\n",
        "  input = (label.item() if type(label) == torch.Tensor else label)\n",
        "  return output_mapping[input]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "gu5cSV0eh3Ya",
        "outputId": "819746fb-8366-451d-b18a-d2827d7aa029"
      },
      "source": [
        "image, label = next(iter(train_set))\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o47q6WJLbk7A"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(AlexNet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(1, 32, kernel_size=3, padding=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((3, 3))\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(256 * 3 * 3, 1024),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(1024, 512),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(512, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kApjwPlMwuWH",
        "outputId": "85f01b1d-e1d9-410c-86e6-965f03082602"
      },
      "source": [
        "nn.Linear(512, 10).weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0365, -0.0129,  0.0371,  ..., -0.0060, -0.0230,  0.0264],\n",
              "        [ 0.0105, -0.0341,  0.0229,  ..., -0.0431,  0.0068,  0.0127],\n",
              "        [-0.0403, -0.0291,  0.0144,  ...,  0.0079, -0.0034, -0.0221],\n",
              "        ...,\n",
              "        [ 0.0002,  0.0207, -0.0106,  ...,  0.0053, -0.0351, -0.0268],\n",
              "        [ 0.0155, -0.0439, -0.0008,  ..., -0.0405, -0.0362,  0.0186],\n",
              "        [ 0.0180, -0.0222,  0.0242,  ...,  0.0439,  0.0285,  0.0228]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swlf5EwA-hxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bbb09f-ce2b-439c-ae03-84d9c6802fdf"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AlexNet(num_classes=10).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (x,y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if batch%100==0:\n",
        "    loss, current = loss.item(), batch * len(x)\n",
        "    print(f\"loss: {loss: >7f}[{curret: >5d}/{size: >5d}]\")\n",
        "\n",
        "def _test(dataloader, model):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      pred = model(x)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  print(f\"Test error: \\n Acc: {correct}, Avg Loss:{test_loss}\")\n",
        "\n",
        "epoch = 5\n",
        "for t in range(epoch):\n",
        "  print(f\"Epoch {t+1}\\n-----------------\")\n",
        "  train(train_loader, model, loss_fn, optimizer)\n",
        "  _test(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-----------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test error: \n",
            " Acc: 0.7280333333333333, Avg Loss:0.007594069035847982\n",
            "Epoch 2\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.7930333333333334, Avg Loss:0.00575895200073719\n",
            "Epoch 3\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.82915, Avg Loss:0.0048978009685873986\n",
            "Epoch 4\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8479833333333333, Avg Loss:0.004352654937158028\n",
            "Epoch 5\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.86055, Avg Loss:0.003996558294693629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V4NyQ4W10em",
        "outputId": "cb841067-331e-43b6-af55-165713137866"
      },
      "source": [
        "for par in model.parameters():\n",
        "  print(par)\n",
        "  print(len(par))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[ 0.2055, -0.2132, -0.1550],\n",
            "          [ 0.1608, -0.1092, -0.1976],\n",
            "          [-0.0179, -0.1101, -0.0109]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1945,  0.1182, -0.2940],\n",
            "          [-0.2536, -0.0288,  0.2353],\n",
            "          [ 0.3132,  0.0530,  0.1770]]],\n",
            "\n",
            "\n",
            "        [[[-0.1933, -0.2681,  0.3056],\n",
            "          [ 0.0044, -0.0577, -0.1703],\n",
            "          [-0.1628,  0.2350, -0.2227]]],\n",
            "\n",
            "\n",
            "        [[[-0.2800,  0.3247, -0.2628],\n",
            "          [-0.2807,  0.1655,  0.2105],\n",
            "          [-0.2288, -0.0941, -0.1436]]],\n",
            "\n",
            "\n",
            "        [[[-0.1959, -0.1268, -0.3256],\n",
            "          [ 0.2928, -0.2566,  0.1442],\n",
            "          [ 0.0353, -0.1397,  0.0158]]],\n",
            "\n",
            "\n",
            "        [[[-0.0815, -0.1822,  0.2802],\n",
            "          [-0.1386,  0.0075,  0.1862],\n",
            "          [-0.2207,  0.1834, -0.2657]]],\n",
            "\n",
            "\n",
            "        [[[-0.3254, -0.1510, -0.1264],\n",
            "          [ 0.2009,  0.3057,  0.2362],\n",
            "          [ 0.3195,  0.1984, -0.2392]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0897, -0.0905,  0.3261],\n",
            "          [ 0.1256, -0.2986,  0.0853],\n",
            "          [-0.3006, -0.2460, -0.2241]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1046,  0.2689,  0.0155],\n",
            "          [ 0.2005, -0.1470, -0.2806],\n",
            "          [-0.2229, -0.0755, -0.3039]]],\n",
            "\n",
            "\n",
            "        [[[-0.1635, -0.3274, -0.2682],\n",
            "          [-0.1378,  0.2841,  0.1219],\n",
            "          [ 0.1367, -0.0213, -0.0114]]],\n",
            "\n",
            "\n",
            "        [[[-0.1142, -0.0788,  0.0604],\n",
            "          [-0.2785,  0.0343,  0.0926],\n",
            "          [-0.3292,  0.1612,  0.2058]]],\n",
            "\n",
            "\n",
            "        [[[-0.2689,  0.0926, -0.0485],\n",
            "          [ 0.0131,  0.3309,  0.0657],\n",
            "          [-0.0463,  0.2293,  0.3254]]],\n",
            "\n",
            "\n",
            "        [[[-0.0015, -0.1084,  0.1560],\n",
            "          [ 0.1994, -0.2942,  0.2470],\n",
            "          [-0.0120, -0.2509,  0.0663]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2268, -0.3000, -0.0431],\n",
            "          [-0.1211, -0.2199, -0.1684],\n",
            "          [-0.1118,  0.2521,  0.0325]]],\n",
            "\n",
            "\n",
            "        [[[-0.1548, -0.2651, -0.0058],\n",
            "          [ 0.2128, -0.0044, -0.1829],\n",
            "          [-0.2364,  0.1539, -0.1785]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2751, -0.0543, -0.3361],\n",
            "          [ 0.1231, -0.3040,  0.0501],\n",
            "          [ 0.2338,  0.1899, -0.0716]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1345,  0.2243, -0.0022],\n",
            "          [-0.0488, -0.0731, -0.2634],\n",
            "          [ 0.3274,  0.2222, -0.0760]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1386, -0.2118, -0.2921],\n",
            "          [ 0.1530,  0.2215,  0.2381],\n",
            "          [-0.2383, -0.1640,  0.1756]]],\n",
            "\n",
            "\n",
            "        [[[-0.2231,  0.2606,  0.3220],\n",
            "          [-0.2681,  0.2130,  0.0305],\n",
            "          [ 0.1686, -0.2626, -0.3105]]],\n",
            "\n",
            "\n",
            "        [[[-0.3122, -0.1262, -0.0511],\n",
            "          [ 0.1865, -0.1272, -0.0452],\n",
            "          [ 0.0910, -0.1776, -0.3162]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3072,  0.3634,  0.1199],\n",
            "          [ 0.2427,  0.2389,  0.2918],\n",
            "          [ 0.2090, -0.0267,  0.1407]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2373,  0.1786,  0.0587],\n",
            "          [-0.3161, -0.2298, -0.2280],\n",
            "          [-0.0760, -0.2373, -0.0283]]],\n",
            "\n",
            "\n",
            "        [[[-0.1995, -0.2233,  0.0661],\n",
            "          [ 0.1830,  0.2680, -0.2216],\n",
            "          [ 0.3249, -0.0490, -0.1504]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1772, -0.0702,  0.0585],\n",
            "          [ 0.3078,  0.3139,  0.1442],\n",
            "          [ 0.3076,  0.2218,  0.2574]]],\n",
            "\n",
            "\n",
            "        [[[-0.0312,  0.2148,  0.0088],\n",
            "          [-0.1611, -0.2081, -0.3229],\n",
            "          [-0.2398, -0.0979,  0.1514]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3070,  0.0240,  0.0227],\n",
            "          [-0.3100,  0.0093,  0.3409],\n",
            "          [ 0.0690,  0.1346, -0.0255]]],\n",
            "\n",
            "\n",
            "        [[[-0.0545, -0.1075,  0.0642],\n",
            "          [-0.0094, -0.1395,  0.2459],\n",
            "          [ 0.3193, -0.2725, -0.3186]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2257,  0.3169,  0.2307],\n",
            "          [ 0.1934,  0.1404, -0.2650],\n",
            "          [ 0.3006, -0.1707, -0.1035]]],\n",
            "\n",
            "\n",
            "        [[[-0.0944, -0.1980,  0.2383],\n",
            "          [-0.1083,  0.0487, -0.2982],\n",
            "          [ 0.1217,  0.2653,  0.1023]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2537, -0.1466,  0.1093],\n",
            "          [ 0.2045,  0.2681,  0.2855],\n",
            "          [-0.2087,  0.1005,  0.1378]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2405,  0.1675, -0.1545],\n",
            "          [ 0.2084, -0.1223, -0.1058],\n",
            "          [ 0.1905, -0.3008, -0.0238]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3163,  0.2841, -0.0795],\n",
            "          [-0.0138,  0.1412, -0.1945],\n",
            "          [-0.2845, -0.1045, -0.0571]]]], device='cuda:0', requires_grad=True)\n",
            "32\n",
            "Parameter containing:\n",
            "tensor([-0.1710,  0.1581,  0.1281,  0.2448, -0.2266, -0.0654,  0.2503, -0.0558,\n",
            "        -0.1721, -0.2902,  0.1764, -0.2132, -0.1408, -0.2030, -0.1300,  0.1700,\n",
            "        -0.2628,  0.2365,  0.2341, -0.0680, -0.0472,  0.2764, -0.0857, -0.1432,\n",
            "         0.0570, -0.2539, -0.0117,  0.2945, -0.2988, -0.2052, -0.1798, -0.2644],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "32\n",
            "Parameter containing:\n",
            "tensor([[[[-3.3937e-02,  2.3653e-02, -3.5269e-02, -2.0758e-02, -1.7931e-02],\n",
            "          [-1.1244e-02,  2.3331e-02,  2.3128e-02, -2.8161e-02, -2.7222e-02],\n",
            "          [-1.9987e-03, -9.1610e-03,  5.8218e-03,  2.6319e-02, -1.6075e-02],\n",
            "          [ 6.8783e-03,  9.6920e-03, -2.7510e-02,  5.9007e-03, -1.4736e-02],\n",
            "          [ 2.3851e-02, -3.3287e-02, -2.7171e-02, -4.9729e-03, -5.6677e-03]],\n",
            "\n",
            "         [[-3.5243e-02,  9.7061e-03,  8.2311e-03,  5.3387e-03,  1.6608e-02],\n",
            "          [ 3.3906e-02,  3.5203e-04,  2.9510e-02, -2.2615e-04, -3.5440e-02],\n",
            "          [ 2.2100e-02,  8.4725e-03,  2.2685e-02, -3.3384e-02, -2.1290e-02],\n",
            "          [-1.5875e-03,  2.0048e-02, -2.4291e-02,  7.7404e-03,  2.1485e-02],\n",
            "          [-6.3561e-03, -2.2421e-02,  1.1895e-02,  3.2036e-02,  1.1766e-02]],\n",
            "\n",
            "         [[-2.0299e-02,  1.4042e-02,  2.1714e-03,  6.6347e-03,  2.3646e-02],\n",
            "          [-5.6843e-03, -1.4368e-02,  7.3669e-03,  3.1242e-02,  2.3211e-02],\n",
            "          [-1.7967e-02, -8.4791e-03, -1.2549e-02,  2.9541e-03, -3.3230e-02],\n",
            "          [ 1.6838e-03,  3.1398e-02,  2.0647e-02, -8.8910e-03, -3.4499e-02],\n",
            "          [ 1.9568e-03, -9.2920e-03,  2.9107e-02, -2.0567e-02, -1.4832e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6181e-02,  1.0227e-03, -1.1627e-02, -1.4051e-02, -1.3402e-02],\n",
            "          [-3.1475e-02,  6.3074e-03,  7.5118e-03,  5.6649e-04,  1.8844e-02],\n",
            "          [ 6.0648e-03, -1.7123e-03, -3.1528e-02,  1.5455e-02, -3.3385e-02],\n",
            "          [-3.2308e-02, -1.1288e-02,  2.4485e-02, -1.6240e-02,  2.1382e-02],\n",
            "          [-2.0509e-02, -3.2130e-02, -2.6441e-02,  6.0534e-03,  3.5560e-02]],\n",
            "\n",
            "         [[ 4.2476e-03, -2.5282e-02,  5.3417e-03,  8.3708e-03, -2.3564e-02],\n",
            "          [-2.2309e-02, -2.5774e-02, -3.3405e-02, -1.5695e-03, -1.1760e-02],\n",
            "          [ 5.8888e-03,  9.4918e-03, -2.5712e-02, -3.4060e-03, -3.2218e-02],\n",
            "          [-5.1148e-05, -1.9495e-02,  2.3759e-02, -3.3622e-03, -3.8038e-03],\n",
            "          [-3.5023e-02, -1.2089e-02, -1.7929e-02, -2.3096e-02,  1.9203e-02]],\n",
            "\n",
            "         [[ 1.8962e-02, -3.5194e-02,  2.9538e-02,  1.6531e-02, -1.3095e-02],\n",
            "          [ 1.6432e-02,  2.2111e-03,  1.0156e-02,  3.1044e-02, -3.5261e-02],\n",
            "          [ 3.2210e-02,  2.5632e-02,  3.9806e-03,  8.7473e-03,  2.8719e-02],\n",
            "          [-3.4607e-02,  2.4034e-02,  2.4405e-02,  3.2705e-02,  1.4852e-02],\n",
            "          [-1.2265e-02, -3.7690e-03,  4.6598e-04,  9.1470e-03,  8.3973e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2906e-02, -1.4799e-02,  2.2762e-03,  8.0066e-03,  3.1864e-02],\n",
            "          [-1.7983e-02, -1.8452e-02, -3.1055e-02, -3.1650e-02, -9.4957e-03],\n",
            "          [ 1.1758e-02, -1.0749e-02, -7.2130e-03, -2.6304e-02,  1.3902e-02],\n",
            "          [ 7.8799e-03, -2.8489e-02, -1.3679e-02, -3.4161e-02,  1.8376e-02],\n",
            "          [ 1.2130e-02,  1.1447e-02, -1.8573e-02,  2.7898e-02,  1.4484e-02]],\n",
            "\n",
            "         [[-2.0477e-03, -4.4052e-03,  2.1002e-02, -8.7839e-03,  3.7056e-03],\n",
            "          [-2.9937e-02, -9.6774e-03,  2.5475e-02, -7.0053e-03, -1.7551e-02],\n",
            "          [-3.2745e-03, -3.3019e-02,  3.0512e-02,  2.1085e-04, -7.0769e-03],\n",
            "          [-1.3558e-02, -2.4780e-02,  1.0027e-02,  1.8726e-04, -8.7074e-03],\n",
            "          [ 3.3401e-02,  6.2932e-03,  3.1913e-02, -1.3947e-02,  2.9152e-02]],\n",
            "\n",
            "         [[ 1.2359e-02,  2.1258e-02,  1.0783e-02, -2.1535e-02,  1.3209e-02],\n",
            "          [ 1.0319e-02,  2.8661e-02, -1.9908e-02, -1.1566e-02,  9.1108e-03],\n",
            "          [ 2.9788e-02, -1.0715e-02,  3.3259e-02, -2.0737e-02, -2.4059e-02],\n",
            "          [ 2.9423e-02,  5.5598e-03,  7.2579e-03,  1.6263e-02,  1.4158e-02],\n",
            "          [-7.9216e-04,  1.3170e-02, -2.1007e-02,  2.0733e-02,  3.3061e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9831e-02,  6.7140e-03, -2.0806e-02, -2.0819e-03,  6.1502e-03],\n",
            "          [-5.0662e-03,  1.6047e-02, -3.5542e-02, -2.8833e-02, -2.9703e-02],\n",
            "          [ 1.7306e-03, -3.0805e-02,  1.4449e-02,  2.4175e-02, -1.9110e-02],\n",
            "          [ 1.9963e-02, -2.4081e-02,  2.8256e-03,  4.0626e-03, -2.6593e-03],\n",
            "          [ 2.7428e-02,  1.7264e-02, -2.7166e-02, -3.3039e-02, -1.2583e-02]],\n",
            "\n",
            "         [[ 1.3662e-02, -7.8891e-03,  2.6215e-02,  1.2165e-02,  2.5379e-02],\n",
            "          [-2.6790e-03, -1.6472e-03,  1.0024e-02, -4.5678e-03, -2.5987e-02],\n",
            "          [-2.9771e-02,  4.2812e-03,  4.2114e-03,  7.6665e-04,  1.4130e-02],\n",
            "          [-3.4762e-02, -3.3755e-02,  3.4636e-03, -4.9957e-03, -2.0719e-02],\n",
            "          [ 4.9739e-03,  9.9133e-03, -8.5282e-03, -3.0510e-02,  2.3783e-02]],\n",
            "\n",
            "         [[-2.4822e-02,  9.1780e-03, -1.0364e-02,  1.8746e-03, -5.2576e-03],\n",
            "          [-1.3897e-02, -2.2770e-02,  2.0114e-03, -7.2692e-03, -1.7943e-02],\n",
            "          [ 2.4738e-02,  2.9475e-02, -8.5287e-03, -2.4387e-02, -2.2017e-02],\n",
            "          [ 7.6363e-03,  2.6238e-02, -9.6208e-03,  2.7564e-02,  2.9220e-02],\n",
            "          [-1.0822e-03, -2.3102e-02, -2.7797e-02,  3.1815e-02, -1.2600e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 8.2140e-03, -1.8519e-02, -1.5671e-02, -2.0794e-02,  3.1651e-02],\n",
            "          [-3.1577e-02, -3.2503e-02, -6.8331e-03, -2.5018e-02,  2.3700e-02],\n",
            "          [ 1.0832e-02, -1.2575e-02, -1.4657e-02,  2.6350e-02,  3.3287e-02],\n",
            "          [-3.1366e-02, -2.4811e-02, -2.8640e-02, -2.1622e-02,  1.6158e-02],\n",
            "          [-4.0759e-03,  5.6935e-03, -9.2133e-03,  5.3132e-03,  2.3538e-02]],\n",
            "\n",
            "         [[ 3.2098e-02,  1.6631e-02, -3.0162e-03, -2.9784e-02, -2.9809e-02],\n",
            "          [ 1.9935e-03, -1.9773e-02, -2.8491e-02,  2.3513e-02, -3.5137e-02],\n",
            "          [ 2.9201e-02, -1.1782e-02, -2.7888e-02,  1.7087e-02,  1.3656e-02],\n",
            "          [ 2.6360e-02,  3.3338e-03, -3.2838e-02, -3.4944e-02,  1.4632e-02],\n",
            "          [-2.4890e-02, -1.4793e-02, -1.8229e-02, -1.5307e-02, -1.8877e-02]],\n",
            "\n",
            "         [[-1.8748e-03,  2.9447e-02, -1.5307e-02, -2.8053e-02, -2.4367e-02],\n",
            "          [-7.9170e-03, -1.8573e-03,  2.7189e-02,  3.0346e-02,  6.7869e-03],\n",
            "          [ 2.8795e-02,  1.7249e-02, -3.0223e-02, -2.6531e-02,  5.0500e-03],\n",
            "          [-2.0115e-02,  2.8200e-02,  4.7323e-03,  3.1131e-02,  2.1628e-03],\n",
            "          [ 1.0231e-02, -1.9556e-02,  2.8883e-02, -1.3462e-02,  3.3796e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.0453e-02,  2.6659e-02,  1.1468e-02,  3.0982e-02,  1.0032e-02],\n",
            "          [ 3.1923e-02,  1.5497e-02, -2.8643e-02,  2.5406e-02, -1.2499e-02],\n",
            "          [ 3.4559e-02,  3.5161e-02, -2.4928e-02, -3.2622e-02,  3.5538e-04],\n",
            "          [ 1.8685e-02, -3.2763e-02, -1.6326e-02,  3.3586e-02, -6.4786e-03],\n",
            "          [-5.2293e-03,  7.1192e-03,  3.2483e-02, -2.2437e-04, -1.5245e-02]],\n",
            "\n",
            "         [[-3.1171e-02,  3.0693e-02, -1.5596e-02,  3.9924e-03, -1.8510e-02],\n",
            "          [ 1.0923e-02,  3.2691e-02,  2.1264e-02,  7.8405e-03, -1.6106e-02],\n",
            "          [-2.2877e-02,  1.3571e-02, -1.0881e-04, -3.2931e-03, -8.5087e-03],\n",
            "          [ 1.4762e-02,  4.4973e-03,  5.0566e-03, -2.2355e-03,  3.3380e-02],\n",
            "          [ 1.0671e-02,  2.5215e-02, -1.8149e-02,  2.9837e-02, -1.9715e-02]],\n",
            "\n",
            "         [[ 5.6022e-03,  2.9813e-03, -2.8097e-02,  5.5825e-04,  4.7200e-03],\n",
            "          [ 1.7557e-03,  8.1864e-03, -3.0601e-02, -1.2452e-02,  3.4783e-02],\n",
            "          [ 3.4899e-02, -8.1850e-03, -8.5293e-03, -3.4500e-02,  1.1849e-02],\n",
            "          [ 3.2996e-03,  1.4998e-02, -2.7404e-02,  3.5942e-02, -4.1669e-03],\n",
            "          [ 1.4436e-02, -8.9391e-03, -7.0665e-04,  3.0113e-02,  3.1904e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.7311e-02, -6.9705e-03,  2.7105e-02,  8.7982e-03,  2.9828e-02],\n",
            "          [ 6.3604e-03,  2.2706e-03,  2.9003e-02, -4.4376e-03, -9.9466e-03],\n",
            "          [ 1.6299e-02, -3.4724e-02, -1.2241e-02, -1.1114e-02, -3.0062e-02],\n",
            "          [-2.8909e-02, -1.1548e-02, -4.0357e-03, -2.6531e-02,  1.3527e-02],\n",
            "          [-2.3330e-02,  6.1734e-03, -8.3961e-03,  3.2316e-03, -3.0789e-02]],\n",
            "\n",
            "         [[ 3.4317e-02, -2.1082e-02, -2.1628e-02,  1.7413e-02,  2.4747e-02],\n",
            "          [ 1.8782e-02,  3.1344e-02, -1.0277e-02,  2.2616e-02,  2.5701e-02],\n",
            "          [-2.6168e-02,  1.7477e-02,  2.1261e-02,  1.5606e-02, -2.5289e-02],\n",
            "          [-2.3845e-02,  1.6092e-02, -3.1833e-02,  1.0741e-03,  1.3964e-02],\n",
            "          [ 2.3792e-02, -3.5100e-02, -3.7625e-03,  1.1092e-03, -1.3331e-02]],\n",
            "\n",
            "         [[ 1.0463e-02,  2.2647e-02,  3.2514e-02, -3.4092e-03,  2.7185e-02],\n",
            "          [-8.7639e-03,  7.7638e-03, -2.0893e-02, -3.4599e-02,  1.1809e-02],\n",
            "          [-3.5123e-02, -6.9310e-03,  1.8016e-02, -3.0284e-02, -3.1832e-02],\n",
            "          [-3.4421e-02,  1.9133e-02,  2.9640e-02, -1.2360e-02,  1.6964e-02],\n",
            "          [-1.3844e-02,  3.2049e-02, -5.5702e-03,  2.4946e-02,  2.2575e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.6098e-02,  3.2919e-03, -2.2099e-03, -7.6073e-03,  2.9768e-02],\n",
            "          [-8.8475e-03, -8.6788e-03, -2.5161e-02,  3.1988e-02, -2.1116e-02],\n",
            "          [ 6.8267e-03,  9.7818e-03, -1.6627e-02, -2.4305e-02, -3.3492e-02],\n",
            "          [-1.3890e-02, -2.4382e-02,  3.2947e-02,  4.1604e-03,  1.9077e-03],\n",
            "          [-1.9449e-02,  9.2679e-04,  8.2985e-04,  2.1938e-02,  1.7160e-02]],\n",
            "\n",
            "         [[-2.3337e-02, -3.5540e-03, -1.1375e-04,  2.2825e-02,  3.7608e-02],\n",
            "          [ 1.0361e-02,  2.8606e-02, -2.9796e-02, -1.6018e-04, -1.4508e-03],\n",
            "          [-2.8064e-02, -1.9587e-02, -2.3062e-02,  2.3078e-02,  3.4438e-02],\n",
            "          [-2.0663e-02, -2.3557e-02, -1.2599e-02, -2.0347e-02,  6.3726e-03],\n",
            "          [-2.0877e-02,  7.3233e-03, -3.5307e-02, -2.4341e-02, -3.0979e-02]],\n",
            "\n",
            "         [[-9.9974e-03,  2.6110e-02, -1.5759e-02,  1.6115e-02, -2.0672e-02],\n",
            "          [ 1.3787e-02, -1.6997e-02,  3.4115e-02,  6.8180e-03, -1.0774e-02],\n",
            "          [ 8.8869e-03,  1.2114e-03,  1.5896e-02, -2.1524e-02, -7.6937e-03],\n",
            "          [-1.0092e-02,  3.3846e-02,  2.4997e-02,  3.1715e-02, -3.3795e-02],\n",
            "          [-3.4305e-02, -1.9989e-02, -2.1792e-02, -1.0041e-02, -2.6266e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.0386e-02,  3.3547e-02,  1.9897e-02,  1.4609e-02, -2.0159e-02],\n",
            "          [ 8.8470e-03,  2.2389e-02,  9.2988e-03, -3.6554e-03,  2.5153e-02],\n",
            "          [-3.3708e-02, -7.3175e-03,  3.5712e-02, -2.5712e-02, -1.5544e-02],\n",
            "          [-6.0873e-03, -3.0898e-02,  2.0612e-02, -2.2511e-02,  3.0585e-02],\n",
            "          [-2.5433e-03, -9.5974e-03,  1.1085e-02, -3.1118e-02, -1.4088e-02]],\n",
            "\n",
            "         [[ 2.8025e-02, -3.3882e-02, -4.3773e-03,  1.1091e-02,  1.4091e-02],\n",
            "          [-3.0494e-02,  2.1417e-03,  1.2274e-02,  2.2534e-02, -1.9365e-02],\n",
            "          [-1.8302e-02, -3.8425e-03,  2.7352e-02, -2.1319e-02, -1.9208e-02],\n",
            "          [ 3.6305e-03,  3.0770e-02, -2.9164e-02,  2.7908e-02, -8.8414e-04],\n",
            "          [ 1.7234e-02, -8.0559e-03, -1.4234e-02,  8.6550e-03, -3.8150e-03]],\n",
            "\n",
            "         [[ 3.4779e-02, -1.9776e-02,  1.0971e-02, -8.3958e-03, -2.7790e-02],\n",
            "          [ 5.0662e-03, -3.1000e-02, -6.5206e-03, -9.2835e-03, -1.3310e-02],\n",
            "          [-2.7475e-02,  1.0985e-02, -2.0872e-02, -1.5776e-02, -3.2807e-03],\n",
            "          [-2.7545e-02,  1.3591e-02, -9.6627e-04,  1.8501e-02, -1.7611e-02],\n",
            "          [-9.3122e-03,  3.5821e-03,  2.7201e-02,  2.7283e-02,  3.3842e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.5542e-03,  3.8384e-02,  6.0873e-03, -5.8539e-03,  2.3596e-02],\n",
            "          [ 1.8071e-02,  2.2800e-02, -2.2854e-03, -2.3936e-02,  2.0475e-02],\n",
            "          [-2.6797e-02, -8.8834e-03,  1.7746e-03, -3.0706e-02,  2.4671e-02],\n",
            "          [ 3.9307e-02, -2.5553e-03,  3.1533e-02,  1.6590e-02, -1.1709e-02],\n",
            "          [ 3.4018e-02, -2.0038e-02,  1.5592e-02, -3.2744e-02, -7.8253e-03]],\n",
            "\n",
            "         [[-2.6016e-02, -2.5869e-03, -2.1102e-02, -3.4805e-02, -2.6267e-02],\n",
            "          [-2.8964e-02,  2.4003e-02,  7.8174e-03,  6.9778e-03, -1.3343e-02],\n",
            "          [-2.4610e-02,  3.1215e-02,  3.4949e-02,  7.6056e-03, -5.2147e-03],\n",
            "          [-1.0345e-02, -5.9653e-03,  1.9929e-02,  5.7068e-03,  3.8851e-03],\n",
            "          [ 9.6430e-03,  4.1590e-02,  3.1554e-02,  2.9846e-02, -1.6016e-02]],\n",
            "\n",
            "         [[ 3.3295e-02, -2.9550e-02,  2.4286e-03,  1.8338e-02,  1.8625e-02],\n",
            "          [-4.4585e-03,  2.4238e-03, -9.0779e-03,  2.2648e-02, -2.6090e-02],\n",
            "          [ 1.7686e-03, -1.5109e-02,  2.2563e-02,  2.8350e-02,  9.7792e-03],\n",
            "          [ 1.5237e-02,  1.8470e-02,  2.6514e-02, -1.1290e-02, -8.0800e-03],\n",
            "          [-1.2623e-02,  2.1296e-02,  1.5468e-02, -3.1898e-02, -2.6839e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.9633e-03,  2.5760e-02,  4.3586e-03,  2.8476e-02, -2.5834e-02],\n",
            "          [-2.5752e-03,  1.4538e-02,  2.4596e-02, -2.2794e-02, -6.5554e-03],\n",
            "          [ 2.3693e-02,  1.0633e-02, -3.2273e-02, -3.3930e-02, -6.7889e-03],\n",
            "          [-1.0715e-02,  1.6762e-03, -6.8473e-03, -2.1302e-03,  2.5312e-02],\n",
            "          [ 3.3369e-02, -2.4463e-02,  6.2076e-03, -1.9284e-02, -5.2545e-03]],\n",
            "\n",
            "         [[-1.5188e-02, -2.1724e-03,  3.4582e-02,  2.2351e-02,  8.0022e-03],\n",
            "          [ 2.0242e-02, -1.8170e-02,  1.2414e-02,  9.9311e-03, -4.1093e-03],\n",
            "          [-3.5629e-03, -4.8350e-03,  4.5495e-03, -5.7811e-03, -2.8165e-03],\n",
            "          [ 3.2633e-03,  3.3221e-02,  2.5528e-02, -2.3398e-02,  3.6672e-02],\n",
            "          [-3.0623e-02,  1.4684e-02,  3.0741e-02, -6.5100e-04,  3.5539e-02]],\n",
            "\n",
            "         [[ 2.6915e-02,  2.0735e-02,  9.7167e-03, -2.1555e-02, -2.7360e-02],\n",
            "          [-1.4679e-02,  1.3616e-02,  3.7992e-03,  2.5897e-02,  2.0377e-02],\n",
            "          [-3.2353e-02, -1.0864e-02,  2.7126e-02, -1.9880e-02, -2.4540e-02],\n",
            "          [-2.3772e-02,  3.1524e-03, -2.5291e-02,  8.4170e-03,  1.6150e-04],\n",
            "          [-2.3789e-02,  4.3820e-04, -2.8332e-02,  6.8325e-03,  4.7673e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5014e-02,  2.6698e-02, -2.0777e-02, -1.2924e-02, -2.0635e-02],\n",
            "          [ 3.1241e-02, -9.9766e-03,  1.9829e-02, -8.8755e-03, -3.2624e-02],\n",
            "          [-1.8815e-02, -1.4921e-02, -1.8904e-02,  2.9557e-02,  3.5005e-02],\n",
            "          [-3.3503e-02,  2.1465e-02,  7.5900e-04, -2.6534e-02,  3.1134e-03],\n",
            "          [-1.6012e-02, -1.0370e-02,  2.5527e-02,  2.4769e-02, -2.5674e-02]],\n",
            "\n",
            "         [[ 2.2017e-02, -2.8498e-02, -3.2789e-02, -3.0653e-02,  1.9705e-02],\n",
            "          [ 1.7576e-02,  2.4488e-02,  3.4556e-02,  3.2018e-02,  3.5203e-03],\n",
            "          [-3.3320e-02,  2.0539e-02,  4.9390e-03,  2.6436e-03, -3.0677e-02],\n",
            "          [-2.6501e-02,  2.6743e-02,  2.7489e-02,  1.6731e-02, -3.2855e-02],\n",
            "          [ 9.7446e-03,  8.7286e-03,  1.9750e-02, -3.4414e-02, -2.5383e-02]],\n",
            "\n",
            "         [[ 2.5356e-02,  3.5288e-02, -2.7284e-02,  7.2179e-03,  2.9643e-02],\n",
            "          [ 3.6857e-02, -2.3022e-03,  4.6071e-03,  1.9946e-02, -2.8049e-02],\n",
            "          [ 1.5724e-02,  2.5368e-02,  6.7148e-03,  2.8596e-02, -2.3042e-02],\n",
            "          [ 2.0782e-02, -3.3402e-03,  1.5001e-02,  1.5306e-03,  2.2222e-03],\n",
            "          [-1.1720e-02,  1.4324e-02, -2.0578e-02, -3.4275e-02,  3.9160e-03]]]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "64\n",
            "Parameter containing:\n",
            "tensor([-0.0186, -0.0108,  0.0070, -0.0251,  0.0053,  0.0195,  0.0260, -0.0262,\n",
            "        -0.0319,  0.0034, -0.0248,  0.0086, -0.0339,  0.0163, -0.0115,  0.0156,\n",
            "         0.0287, -0.0069, -0.0188, -0.0037, -0.0120, -0.0162, -0.0133,  0.0260,\n",
            "         0.0021, -0.0257,  0.0220, -0.0086, -0.0238,  0.0162, -0.0303,  0.0241,\n",
            "        -0.0195, -0.0022,  0.0046, -0.0179, -0.0248,  0.0288,  0.0260,  0.0011,\n",
            "        -0.0299,  0.0117, -0.0181,  0.0242, -0.0087, -0.0010, -0.0183,  0.0183,\n",
            "         0.0108, -0.0317, -0.0004, -0.0225,  0.0256, -0.0299, -0.0339,  0.0026,\n",
            "        -0.0150, -0.0204, -0.0326,  0.0114, -0.0091,  0.0294, -0.0306,  0.0025],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "64\n",
            "Parameter containing:\n",
            "tensor([[[[-3.7501e-02,  3.9859e-02, -2.6610e-02],\n",
            "          [ 1.4850e-02,  6.0153e-03,  3.7592e-02],\n",
            "          [-3.5928e-03, -1.2977e-02, -9.4926e-03]],\n",
            "\n",
            "         [[ 2.1286e-02,  3.8856e-02,  2.0338e-02],\n",
            "          [ 2.1659e-02,  3.7168e-03,  1.6566e-02],\n",
            "          [ 1.4525e-02, -2.5982e-02,  3.4791e-02]],\n",
            "\n",
            "         [[ 3.0634e-02, -2.6156e-02, -7.6890e-03],\n",
            "          [-1.3596e-02, -1.0024e-02, -1.1816e-02],\n",
            "          [-2.5396e-02, -2.8552e-02,  3.9416e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6921e-02, -2.6891e-02, -3.7675e-03],\n",
            "          [-2.3121e-02, -1.2335e-02,  2.3164e-02],\n",
            "          [ 2.4145e-02, -7.2000e-03,  2.6561e-02]],\n",
            "\n",
            "         [[-2.1168e-02,  1.1680e-02,  3.8188e-02],\n",
            "          [ 9.5500e-03, -3.7584e-02, -6.6377e-03],\n",
            "          [-3.0864e-02,  1.7257e-02, -3.7452e-02]],\n",
            "\n",
            "         [[ 2.2379e-03, -2.8750e-02, -2.2452e-02],\n",
            "          [-1.3639e-02,  3.2119e-03, -2.4559e-02],\n",
            "          [ 9.4167e-03,  2.7418e-02,  1.3656e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.2567e-02, -3.3265e-02, -1.5581e-02],\n",
            "          [-5.0382e-03,  1.8611e-02, -1.0755e-02],\n",
            "          [ 2.7281e-02,  1.8872e-02,  2.7891e-02]],\n",
            "\n",
            "         [[-8.1011e-03, -8.1453e-03,  1.5148e-03],\n",
            "          [ 2.3109e-02,  1.5130e-02,  3.8384e-02],\n",
            "          [ 3.6164e-02,  1.0109e-02, -3.3482e-02]],\n",
            "\n",
            "         [[-3.9951e-02, -1.8722e-02,  2.6049e-02],\n",
            "          [-2.5241e-02,  1.2050e-02,  1.7304e-02],\n",
            "          [-1.3131e-02,  1.0542e-03,  3.7178e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.4096e-03, -2.7281e-02, -2.2923e-02],\n",
            "          [-2.5232e-02,  3.7428e-02,  4.0018e-02],\n",
            "          [-3.0138e-02, -1.2027e-02, -3.7305e-02]],\n",
            "\n",
            "         [[ 5.1068e-03,  1.3728e-02,  3.1307e-03],\n",
            "          [-1.1393e-03,  7.2254e-03,  1.1798e-02],\n",
            "          [-4.1465e-02, -4.4419e-03, -1.6556e-03]],\n",
            "\n",
            "         [[ 3.6252e-02, -2.5433e-02, -3.3659e-02],\n",
            "          [-2.9704e-02,  3.7345e-02, -4.7866e-03],\n",
            "          [-5.2286e-03, -1.2109e-02,  1.7066e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.6361e-03,  1.9850e-02,  1.9565e-02],\n",
            "          [-4.0574e-03,  2.6663e-02,  5.9140e-03],\n",
            "          [ 2.5340e-02, -4.0061e-02, -1.4255e-02]],\n",
            "\n",
            "         [[ 8.9365e-03, -3.0367e-02,  1.6349e-02],\n",
            "          [-3.4684e-02,  1.8575e-02,  4.0135e-02],\n",
            "          [-2.0240e-02,  3.5326e-02, -1.0413e-02]],\n",
            "\n",
            "         [[ 2.9146e-02, -3.2303e-02, -6.8538e-03],\n",
            "          [-3.8245e-02,  3.2196e-02,  3.3332e-02],\n",
            "          [ 7.2386e-03,  1.2172e-02, -8.4208e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4608e-02,  4.0818e-02,  1.8884e-02],\n",
            "          [-3.5721e-02,  1.5146e-02, -7.6828e-03],\n",
            "          [ 2.6246e-02, -4.2109e-03,  8.9687e-03]],\n",
            "\n",
            "         [[-1.5949e-02, -3.6931e-02, -1.1127e-02],\n",
            "          [-2.0576e-02,  4.2005e-02, -3.7752e-02],\n",
            "          [ 2.8259e-02, -2.3914e-03,  3.0519e-02]],\n",
            "\n",
            "         [[-4.7129e-03, -1.0541e-02,  2.1520e-02],\n",
            "          [ 3.1255e-02,  2.2622e-02, -2.6725e-02],\n",
            "          [ 1.9202e-02,  2.5643e-02, -2.6227e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.1628e-03,  4.4440e-04,  3.7085e-02],\n",
            "          [ 2.6035e-02, -2.8485e-02, -3.9402e-02],\n",
            "          [ 2.9897e-02, -1.8724e-02, -3.8606e-02]],\n",
            "\n",
            "         [[-3.1759e-02, -2.8244e-02,  2.5862e-02],\n",
            "          [ 2.3639e-02,  3.7241e-02, -6.3841e-03],\n",
            "          [ 1.3935e-02,  3.0107e-02, -4.0641e-02]],\n",
            "\n",
            "         [[-2.6875e-02, -1.3748e-02,  3.5430e-02],\n",
            "          [-3.7145e-02,  3.0155e-02, -2.2870e-02],\n",
            "          [ 3.0687e-02,  1.6707e-02,  3.5100e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.8721e-02,  3.3837e-02, -7.7714e-03],\n",
            "          [ 3.4668e-02, -2.1178e-02,  7.9186e-03],\n",
            "          [ 2.6805e-03,  3.4098e-02,  8.6782e-03]],\n",
            "\n",
            "         [[-2.6192e-02,  1.0860e-02, -1.9210e-02],\n",
            "          [-2.7651e-02, -3.1867e-02, -1.7102e-02],\n",
            "          [ 9.5496e-03,  3.8449e-02, -3.8888e-02]],\n",
            "\n",
            "         [[ 2.4404e-03,  8.3122e-03,  2.8757e-02],\n",
            "          [-1.4477e-02,  3.5992e-02,  3.3971e-02],\n",
            "          [ 2.8666e-02,  4.0075e-03, -1.4642e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.5690e-03,  1.0738e-02, -1.8162e-03],\n",
            "          [ 4.0322e-03, -3.5064e-02, -1.1759e-02],\n",
            "          [-3.9840e-02,  2.7281e-02,  1.1204e-02]],\n",
            "\n",
            "         [[ 3.4427e-02,  1.8674e-02,  5.7550e-03],\n",
            "          [ 2.2920e-02,  3.7648e-02, -3.9074e-02],\n",
            "          [ 1.4576e-02,  3.0176e-02, -8.2394e-03]],\n",
            "\n",
            "         [[ 3.0807e-02, -3.6999e-02,  2.6011e-02],\n",
            "          [-2.7047e-02,  2.1862e-02, -4.1919e-02],\n",
            "          [ 4.1101e-02,  4.9576e-03,  2.0901e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2865e-03, -3.6206e-03, -3.7252e-02],\n",
            "          [ 2.9109e-02, -2.3459e-02,  5.6566e-03],\n",
            "          [ 1.6559e-02, -1.2878e-02,  3.0310e-02]],\n",
            "\n",
            "         [[-2.7766e-03,  3.7756e-02,  3.0057e-03],\n",
            "          [-1.7548e-02,  2.2242e-02, -1.1598e-02],\n",
            "          [ 4.0640e-02, -3.9381e-02, -3.5523e-02]],\n",
            "\n",
            "         [[ 1.7440e-02,  3.0112e-02,  4.0875e-02],\n",
            "          [ 3.5674e-02, -4.8832e-03,  4.3871e-04],\n",
            "          [-1.4532e-02, -3.9118e-02, -3.7084e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.7345e-02,  1.6131e-02, -2.0347e-02],\n",
            "          [-6.5551e-03, -2.3330e-02,  3.7483e-02],\n",
            "          [-9.2374e-03,  8.4578e-03,  2.6696e-02]],\n",
            "\n",
            "         [[-1.3522e-02,  1.6419e-02,  3.9443e-02],\n",
            "          [ 1.1865e-03,  3.2539e-03, -3.8995e-02],\n",
            "          [ 3.3218e-02,  4.1516e-02,  4.2443e-05]],\n",
            "\n",
            "         [[ 9.4722e-04, -2.2378e-02, -3.5633e-02],\n",
            "          [-2.0974e-02, -2.4450e-02, -1.9361e-02],\n",
            "          [ 1.7818e-02, -2.9991e-02, -3.9246e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0199e-02,  2.4304e-02, -8.9477e-03],\n",
            "          [-1.9636e-02,  7.9172e-04,  1.8148e-02],\n",
            "          [-2.1796e-02,  3.7508e-02, -1.2618e-03]],\n",
            "\n",
            "         [[ 7.6885e-03,  3.1052e-02, -3.6104e-02],\n",
            "          [-3.7974e-02, -2.8610e-02, -3.8347e-02],\n",
            "          [ 1.7041e-02,  2.9066e-02,  1.2070e-02]],\n",
            "\n",
            "         [[-3.5754e-02, -3.5390e-02, -4.0563e-02],\n",
            "          [ 5.6784e-03, -8.8764e-03,  3.6105e-02],\n",
            "          [ 4.2107e-02, -7.4885e-03, -2.2128e-04]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "128\n",
            "Parameter containing:\n",
            "tensor([ 0.0219,  0.0204, -0.0213, -0.0099,  0.0065, -0.0056, -0.0050,  0.0081,\n",
            "        -0.0416,  0.0315, -0.0009, -0.0044, -0.0330, -0.0100, -0.0087, -0.0368,\n",
            "         0.0353,  0.0104,  0.0069,  0.0111, -0.0321,  0.0338,  0.0097,  0.0025,\n",
            "         0.0334,  0.0354,  0.0334, -0.0046,  0.0186, -0.0141,  0.0217,  0.0270,\n",
            "         0.0315,  0.0386, -0.0128,  0.0048, -0.0413, -0.0202, -0.0025, -0.0361,\n",
            "         0.0200,  0.0062,  0.0202,  0.0047, -0.0303, -0.0206,  0.0368, -0.0094,\n",
            "        -0.0031,  0.0078,  0.0291, -0.0248,  0.0337,  0.0353, -0.0340, -0.0353,\n",
            "         0.0210, -0.0136,  0.0203, -0.0030, -0.0043,  0.0017, -0.0284, -0.0266,\n",
            "        -0.0201, -0.0294,  0.0004, -0.0340,  0.0395,  0.0322, -0.0219, -0.0126,\n",
            "         0.0132,  0.0019, -0.0011, -0.0376,  0.0167, -0.0264,  0.0184, -0.0015,\n",
            "         0.0058,  0.0040, -0.0376,  0.0233,  0.0376,  0.0057,  0.0106,  0.0127,\n",
            "        -0.0151,  0.0215,  0.0178, -0.0229, -0.0366,  0.0345,  0.0306, -0.0261,\n",
            "         0.0246,  0.0377,  0.0162, -0.0248, -0.0171,  0.0211, -0.0294, -0.0205,\n",
            "        -0.0307,  0.0360,  0.0299,  0.0033, -0.0107, -0.0049, -0.0166,  0.0226,\n",
            "         0.0352, -0.0100, -0.0081, -0.0391,  0.0170,  0.0283,  0.0378,  0.0220,\n",
            "        -0.0044, -0.0408,  0.0206,  0.0084,  0.0295,  0.0400, -0.0381,  0.0187],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "128\n",
            "Parameter containing:\n",
            "tensor([[[[ 7.4975e-03, -2.7168e-02,  2.8990e-02],\n",
            "          [ 1.9307e-02, -2.7812e-02, -1.7885e-02],\n",
            "          [ 3.3081e-03,  1.9421e-02, -1.4449e-02]],\n",
            "\n",
            "         [[ 7.3298e-05,  1.2265e-02, -2.5373e-02],\n",
            "          [ 8.6613e-03, -1.0788e-02,  7.4018e-03],\n",
            "          [-1.4376e-03, -1.1642e-02, -1.9582e-02]],\n",
            "\n",
            "         [[-3.2240e-03, -9.1478e-03,  7.7770e-03],\n",
            "          [-2.5988e-02, -1.1222e-02, -1.7390e-02],\n",
            "          [-5.7265e-03,  2.1564e-02,  3.9517e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5195e-02, -2.7128e-02, -2.0459e-02],\n",
            "          [-2.0030e-02,  2.5491e-02,  1.5005e-02],\n",
            "          [-8.4586e-03,  1.5441e-02,  1.4242e-02]],\n",
            "\n",
            "         [[ 1.6871e-02, -2.7289e-02, -1.7038e-02],\n",
            "          [-2.3915e-02, -2.5950e-02,  2.0746e-02],\n",
            "          [-2.1828e-02, -5.9628e-03, -4.1085e-04]],\n",
            "\n",
            "         [[ 2.7657e-03, -1.6452e-02,  1.7040e-02],\n",
            "          [ 2.9156e-02, -2.8260e-02,  7.0302e-03],\n",
            "          [-4.2920e-03, -2.5295e-02, -2.4701e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 6.7660e-03,  1.9579e-02,  5.3363e-03],\n",
            "          [ 2.4473e-02,  5.5604e-03, -4.7167e-03],\n",
            "          [-2.4202e-02, -2.7962e-02, -1.9479e-02]],\n",
            "\n",
            "         [[-1.8588e-03, -1.1824e-02,  2.3482e-02],\n",
            "          [-2.8650e-02,  2.2291e-02, -1.7694e-02],\n",
            "          [-2.5218e-02,  2.8045e-02,  2.1554e-02]],\n",
            "\n",
            "         [[ 2.6104e-02, -1.9646e-02,  2.1000e-02],\n",
            "          [-2.3517e-02, -2.6836e-02,  7.1844e-03],\n",
            "          [ 1.9975e-02,  9.8037e-03,  2.3478e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.5869e-03,  1.3720e-02,  2.4109e-02],\n",
            "          [ 1.1840e-02, -5.2530e-03,  3.3587e-03],\n",
            "          [-1.1808e-02, -2.3341e-02, -2.1147e-02]],\n",
            "\n",
            "         [[-4.1590e-03, -1.3296e-02, -1.2866e-02],\n",
            "          [-3.7562e-03,  6.9099e-03, -2.8367e-02],\n",
            "          [-1.2656e-03, -4.9713e-03,  6.7292e-03]],\n",
            "\n",
            "         [[-2.6663e-03,  1.2666e-02, -1.6922e-02],\n",
            "          [-1.6371e-03,  1.0704e-02, -2.4031e-02],\n",
            "          [ 2.6704e-02, -8.6266e-05, -1.4596e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.8187e-03,  2.1769e-02, -1.1804e-02],\n",
            "          [ 1.7434e-02, -1.9390e-02, -2.1097e-02],\n",
            "          [-1.0656e-02,  1.4536e-02,  6.1428e-03]],\n",
            "\n",
            "         [[-8.5248e-03, -2.5148e-03, -2.9135e-02],\n",
            "          [-1.4647e-02,  1.1642e-02, -1.6586e-02],\n",
            "          [ 8.6835e-04, -3.7325e-03, -1.0955e-02]],\n",
            "\n",
            "         [[ 1.6094e-02, -2.8229e-02,  2.3496e-02],\n",
            "          [-2.5838e-03,  2.8494e-02,  8.5169e-03],\n",
            "          [ 8.0381e-03,  2.7179e-02, -2.0188e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2966e-03, -2.4556e-02, -1.0397e-02],\n",
            "          [ 1.4948e-02, -3.4154e-03, -1.7782e-02],\n",
            "          [ 1.2506e-02,  7.0861e-03,  6.9333e-03]],\n",
            "\n",
            "         [[-1.0397e-03,  8.0319e-03, -1.1616e-03],\n",
            "          [ 1.2118e-02, -5.1466e-03,  1.9536e-02],\n",
            "          [ 5.0654e-03, -2.3323e-03, -7.7204e-03]],\n",
            "\n",
            "         [[-4.5524e-03,  2.2698e-02, -5.8208e-03],\n",
            "          [ 3.9298e-06, -1.5065e-02,  2.1308e-02],\n",
            "          [ 1.6271e-02,  1.7766e-02, -1.9407e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.0351e-02,  1.0023e-02,  1.1964e-02],\n",
            "          [-1.2979e-02, -9.5599e-03, -7.4644e-03],\n",
            "          [-1.5006e-02,  1.1164e-02,  2.3695e-02]],\n",
            "\n",
            "         [[ 2.4941e-02,  1.6129e-02,  2.2030e-02],\n",
            "          [-2.0589e-02, -1.3994e-02, -1.3521e-02],\n",
            "          [ 2.1082e-03, -2.2718e-02, -1.5295e-03]],\n",
            "\n",
            "         [[ 1.3914e-02, -2.7537e-02, -9.8767e-03],\n",
            "          [ 3.0907e-04, -1.5492e-02, -2.8461e-02],\n",
            "          [ 2.0713e-03, -1.1520e-02, -8.4109e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5534e-02,  4.1737e-03,  2.6646e-02],\n",
            "          [ 4.0250e-03,  1.7228e-02, -2.8421e-02],\n",
            "          [ 2.7044e-04,  1.8823e-03,  1.9040e-02]],\n",
            "\n",
            "         [[-1.0571e-02,  1.5579e-02,  1.3787e-02],\n",
            "          [ 1.1734e-02, -5.7888e-03,  7.9114e-03],\n",
            "          [-2.1081e-02, -1.0554e-02,  1.6211e-02]],\n",
            "\n",
            "         [[-9.8536e-03, -7.3502e-03, -6.5425e-03],\n",
            "          [ 1.3911e-02, -2.3781e-02, -1.4194e-02],\n",
            "          [-1.6587e-02,  9.0815e-03, -1.8874e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.8636e-02,  8.2780e-04, -2.2109e-02],\n",
            "          [ 4.1836e-03, -1.2985e-02,  2.2485e-02],\n",
            "          [ 2.7814e-03, -3.8865e-03,  1.8138e-02]],\n",
            "\n",
            "         [[-1.9351e-02,  2.8845e-03,  2.0515e-02],\n",
            "          [ 2.7564e-02,  4.0502e-04,  1.4174e-02],\n",
            "          [ 1.1585e-03, -7.1206e-03, -1.6211e-02]],\n",
            "\n",
            "         [[-6.7494e-04,  3.1820e-03, -2.3197e-02],\n",
            "          [ 6.4251e-03,  6.6604e-03,  5.2131e-05],\n",
            "          [-2.5993e-02,  1.2569e-02, -2.6501e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.8223e-03,  8.5338e-03, -1.3705e-02],\n",
            "          [ 2.5145e-02,  1.1572e-02, -7.4190e-03],\n",
            "          [ 7.7408e-03,  2.7885e-02, -2.9214e-02]],\n",
            "\n",
            "         [[ 7.7242e-03,  2.0659e-02, -4.1501e-03],\n",
            "          [ 1.2880e-02,  1.7463e-02, -6.7471e-03],\n",
            "          [-4.7640e-03,  6.9973e-03, -5.0983e-04]],\n",
            "\n",
            "         [[ 2.0522e-02, -1.8385e-02,  2.4851e-02],\n",
            "          [-2.2721e-02,  1.1334e-02,  2.7850e-02],\n",
            "          [ 1.6465e-02, -2.1706e-02,  4.5347e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8336e-02, -1.2489e-02,  1.2623e-02],\n",
            "          [-2.9096e-02,  1.2309e-02,  2.4252e-02],\n",
            "          [ 6.9338e-03,  2.5472e-03,  1.4660e-02]],\n",
            "\n",
            "         [[-1.6069e-02,  5.3112e-03, -4.9391e-03],\n",
            "          [ 2.0252e-02,  1.9314e-02, -2.2580e-02],\n",
            "          [ 5.0420e-03,  1.4470e-02,  3.2614e-03]],\n",
            "\n",
            "         [[-2.1941e-02, -2.1025e-02,  8.3342e-03],\n",
            "          [-2.1747e-02, -1.8704e-02,  2.8854e-02],\n",
            "          [-1.3935e-02, -1.6822e-02, -2.3969e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8280e-02,  3.8388e-03, -6.9838e-03],\n",
            "          [ 1.6880e-02,  9.4853e-03,  2.8204e-02],\n",
            "          [-2.7177e-02,  1.7736e-02,  2.1673e-02]],\n",
            "\n",
            "         [[ 2.5904e-02,  8.5499e-05, -9.2958e-03],\n",
            "          [ 2.3065e-03,  5.4155e-04, -8.6875e-03],\n",
            "          [-9.6521e-03, -8.1471e-03, -2.0793e-02]],\n",
            "\n",
            "         [[ 1.5978e-03, -6.7165e-03, -1.4065e-02],\n",
            "          [ 2.0794e-03,  9.6878e-03,  1.3334e-02],\n",
            "          [ 2.7911e-02,  5.7590e-03, -5.6111e-03]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "256\n",
            "Parameter containing:\n",
            "tensor([-1.5134e-02, -3.4708e-03,  2.3870e-02, -2.1275e-02,  1.5960e-02,\n",
            "        -1.0444e-02,  2.1401e-02,  2.8014e-02,  6.4514e-04, -4.1436e-03,\n",
            "        -1.0912e-03, -2.8178e-03,  2.7940e-02, -3.3671e-03, -2.9222e-02,\n",
            "         2.1180e-02,  4.9008e-03,  2.5784e-02, -2.9149e-02, -1.1068e-02,\n",
            "         1.4250e-02, -1.4329e-02, -7.9278e-03,  9.0687e-03, -2.1398e-02,\n",
            "        -6.6620e-03,  2.2355e-02, -1.2268e-02,  8.4458e-03, -5.8632e-03,\n",
            "         5.9440e-03, -2.6495e-02, -2.6959e-02, -4.4301e-03, -8.7978e-03,\n",
            "         1.7966e-03,  2.9327e-02, -2.1682e-03,  2.3142e-02,  1.5733e-02,\n",
            "        -2.7219e-02,  1.8891e-02, -1.6765e-02, -2.8874e-02,  2.3358e-02,\n",
            "         1.7813e-03,  5.7668e-03, -8.8548e-03, -2.7308e-02, -1.7555e-02,\n",
            "         1.8663e-02,  2.9949e-03,  3.2487e-03, -9.7811e-03, -1.3160e-02,\n",
            "        -2.3002e-02,  1.4566e-02,  2.9126e-02,  2.3202e-02, -2.5882e-03,\n",
            "         1.0937e-02, -2.4846e-02, -2.2071e-02,  1.4709e-02, -2.5162e-02,\n",
            "         3.7443e-03,  8.6706e-03, -2.8298e-02, -2.4384e-02,  4.1742e-03,\n",
            "        -1.1521e-02, -1.2004e-02, -1.4360e-02, -1.4811e-02,  1.4695e-03,\n",
            "         7.8665e-03, -2.2359e-03, -9.0020e-03, -2.8539e-02,  2.9430e-02,\n",
            "         1.2656e-02, -5.2525e-04,  5.3187e-04,  1.5730e-02,  2.1159e-02,\n",
            "         2.5405e-02,  1.8871e-02,  1.8306e-02, -1.7808e-03, -8.6440e-03,\n",
            "         2.1928e-02,  2.2964e-02, -2.5287e-02,  2.5457e-02, -1.7946e-02,\n",
            "        -2.5131e-02,  2.7391e-02,  3.9570e-04,  2.3547e-02,  3.3593e-03,\n",
            "         1.1777e-02, -7.1897e-03,  1.0533e-02,  1.0716e-02, -6.6676e-03,\n",
            "        -9.3586e-03,  9.2716e-03, -5.4121e-03, -4.2540e-03,  2.9282e-02,\n",
            "        -2.6361e-02,  2.0565e-02,  9.5134e-03,  9.3019e-03,  8.2563e-03,\n",
            "        -2.9275e-02, -2.4780e-02,  2.4382e-02,  6.3561e-03, -1.7657e-02,\n",
            "        -2.8433e-02, -1.4618e-02,  2.2894e-02,  9.8244e-04,  7.0611e-03,\n",
            "         4.9493e-03,  3.0267e-03, -1.8553e-02, -8.2149e-06,  1.0433e-02,\n",
            "         1.1068e-02, -1.6801e-02, -1.2628e-02,  1.7108e-02,  2.8536e-02,\n",
            "         7.2613e-03, -5.6126e-03,  1.8568e-02,  8.6707e-03,  6.6785e-03,\n",
            "         2.3661e-02, -2.4057e-02,  2.4242e-02,  1.2721e-02, -2.2765e-02,\n",
            "        -2.9874e-03, -4.5347e-03, -2.1638e-02, -2.1369e-02,  5.0608e-03,\n",
            "        -8.1362e-03, -1.8763e-02,  2.1008e-04, -2.0530e-02, -2.1938e-02,\n",
            "         3.2739e-03,  1.5968e-02,  3.4320e-03,  2.7411e-02, -1.0008e-02,\n",
            "        -2.6244e-02,  8.6654e-03, -8.9357e-03, -4.4913e-03, -2.3278e-02,\n",
            "         3.2934e-03, -2.8011e-02, -2.7553e-03,  1.1486e-02, -1.6484e-02,\n",
            "         1.8754e-02,  2.8766e-02, -1.0825e-02, -1.4324e-02, -2.1973e-02,\n",
            "        -2.4378e-02, -2.3816e-02,  1.8687e-02, -4.7896e-03, -2.7858e-02,\n",
            "        -2.2405e-02, -2.7691e-02, -8.7384e-03, -2.4449e-02, -1.1063e-02,\n",
            "         1.4721e-02, -4.8712e-03, -2.0060e-02,  2.8822e-02, -2.7530e-02,\n",
            "         2.0239e-02,  3.3593e-03, -2.4541e-04, -1.2017e-02,  5.2962e-03,\n",
            "        -2.7295e-02,  2.8716e-02, -7.5125e-03,  1.3614e-03, -1.3187e-02,\n",
            "         1.4307e-02, -5.4503e-03,  2.2862e-02, -2.6315e-02,  8.3841e-04,\n",
            "         5.9423e-03,  2.8869e-02,  1.4062e-02,  2.5582e-02,  1.1614e-02,\n",
            "        -3.5348e-03,  2.5589e-03, -1.1654e-02,  9.2464e-03, -2.6417e-02,\n",
            "         9.6476e-04, -2.1086e-02, -7.4412e-04,  2.1172e-02,  1.3075e-02,\n",
            "         1.4583e-02, -1.4574e-02,  2.3830e-02, -5.4147e-03, -2.5109e-02,\n",
            "         2.5065e-02, -5.1532e-03, -7.9406e-03,  1.8115e-02, -2.3564e-02,\n",
            "         2.8651e-02, -4.0716e-03,  2.8984e-02, -1.4905e-02, -1.1238e-02,\n",
            "        -1.8671e-02,  2.3588e-02, -1.0994e-02, -2.7881e-02,  2.4633e-02,\n",
            "         1.0345e-02,  1.1101e-02,  8.3132e-03,  1.3734e-02, -2.4489e-02,\n",
            "         6.4197e-03,  2.7603e-02,  5.2488e-03,  2.3908e-02,  1.9075e-02,\n",
            "        -2.1561e-02,  1.4697e-03, -7.5332e-03,  1.4560e-02, -1.7776e-02,\n",
            "         1.5949e-02], device='cuda:0', requires_grad=True)\n",
            "256\n",
            "Parameter containing:\n",
            "tensor([[[[-1.5410e-02,  8.5411e-03, -2.7287e-03],\n",
            "          [ 1.6368e-03,  5.1029e-03,  1.4924e-02],\n",
            "          [ 1.4403e-02, -6.2103e-03, -1.0873e-02]],\n",
            "\n",
            "         [[-3.4666e-03,  2.3204e-03,  1.3973e-02],\n",
            "          [-4.5503e-03, -1.6531e-02,  1.3413e-02],\n",
            "          [-8.6307e-03,  1.8861e-02, -1.7042e-02]],\n",
            "\n",
            "         [[-9.1130e-04,  1.4017e-02, -8.7936e-03],\n",
            "          [ 1.3964e-02,  7.4892e-03, -6.1981e-03],\n",
            "          [-4.8022e-03,  1.1410e-03, -1.2789e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.7635e-03,  5.5094e-03,  1.8825e-02],\n",
            "          [-4.8219e-03, -1.7605e-02, -8.7196e-03],\n",
            "          [-5.1134e-03,  4.5194e-03,  1.3307e-03]],\n",
            "\n",
            "         [[ 2.0425e-02,  1.2136e-02, -1.9359e-02],\n",
            "          [ 1.3176e-03,  7.1726e-03,  1.0610e-02],\n",
            "          [-4.0816e-04, -3.7917e-03, -1.1325e-02]],\n",
            "\n",
            "         [[-1.0679e-02, -1.0642e-02, -1.9936e-02],\n",
            "          [ 8.7918e-04, -1.3343e-02, -1.9151e-02],\n",
            "          [ 4.6385e-03, -1.9438e-02,  1.6084e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.9001e-03,  9.2295e-03, -2.0476e-03],\n",
            "          [ 1.4597e-02,  1.6272e-03, -1.3127e-03],\n",
            "          [-1.6552e-04, -1.0036e-02,  6.2428e-04]],\n",
            "\n",
            "         [[-1.0075e-02, -1.3531e-02, -7.6327e-03],\n",
            "          [ 1.6049e-02, -1.9536e-02,  1.8781e-02],\n",
            "          [ 1.2929e-02, -1.5342e-02,  7.9383e-03]],\n",
            "\n",
            "         [[ 3.7522e-03,  9.2035e-03, -1.2796e-02],\n",
            "          [-6.7472e-04, -1.9310e-02,  3.5811e-03],\n",
            "          [ 6.6429e-03,  1.6839e-03, -2.8242e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3287e-02, -4.3825e-03,  1.9447e-02],\n",
            "          [ 1.5509e-02,  1.6746e-02,  9.7265e-03],\n",
            "          [-2.6994e-03,  2.5095e-04, -1.9988e-02]],\n",
            "\n",
            "         [[-6.7321e-03, -1.9643e-02,  1.8504e-02],\n",
            "          [-1.9223e-02,  8.9410e-03, -7.1871e-03],\n",
            "          [ 1.3449e-02,  8.7886e-03,  4.2813e-03]],\n",
            "\n",
            "         [[-1.5791e-02, -1.3366e-03, -2.2603e-03],\n",
            "          [ 9.5524e-03, -3.8706e-03,  1.9234e-03],\n",
            "          [-8.5912e-03,  1.5051e-02, -2.1016e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 9.6671e-03, -3.8368e-03, -6.9425e-03],\n",
            "          [ 1.4602e-02,  1.9117e-02,  2.2276e-03],\n",
            "          [-1.8349e-02, -2.9593e-03, -7.3943e-03]],\n",
            "\n",
            "         [[-1.0061e-02, -1.3603e-02, -5.6032e-03],\n",
            "          [-1.7473e-02,  1.3245e-04,  8.3479e-03],\n",
            "          [-4.2528e-03,  1.9272e-02, -8.5672e-03]],\n",
            "\n",
            "         [[ 1.5062e-02,  1.3880e-02,  1.2430e-02],\n",
            "          [-4.6941e-03,  6.2639e-03,  1.5777e-03],\n",
            "          [-6.0566e-03,  9.6954e-03, -1.3240e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2832e-02,  9.3026e-03,  2.0080e-02],\n",
            "          [-2.0641e-02,  2.9135e-03, -7.7383e-03],\n",
            "          [ 9.3488e-03,  7.2949e-03,  6.6923e-04]],\n",
            "\n",
            "         [[ 1.1520e-02,  2.6650e-04,  1.9971e-02],\n",
            "          [-1.8310e-02,  1.6856e-02, -1.0990e-02],\n",
            "          [ 2.0285e-02,  1.5799e-02, -1.5772e-02]],\n",
            "\n",
            "         [[ 5.5755e-03,  1.6643e-02,  1.8570e-02],\n",
            "          [ 3.7873e-03,  1.8801e-02, -6.3476e-03],\n",
            "          [ 8.3404e-03,  7.9851e-03, -1.7670e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 7.0081e-03,  1.6039e-02,  1.2688e-02],\n",
            "          [-1.7666e-02, -7.0990e-03,  4.4765e-03],\n",
            "          [-9.5709e-03, -5.7601e-03, -1.9038e-03]],\n",
            "\n",
            "         [[-1.0810e-02,  1.1860e-02, -1.6905e-02],\n",
            "          [ 5.9049e-03,  1.9750e-02,  8.7363e-03],\n",
            "          [ 1.8743e-02,  9.0056e-03, -9.1360e-03]],\n",
            "\n",
            "         [[ 1.8774e-03, -7.3913e-03,  2.1236e-02],\n",
            "          [-1.0335e-03,  8.5646e-03,  2.5805e-03],\n",
            "          [-1.4355e-02, -2.3081e-03,  1.2416e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6191e-02,  1.3451e-02, -1.8378e-02],\n",
            "          [ 8.0824e-03,  1.9402e-02,  1.4323e-03],\n",
            "          [ 7.0513e-03, -9.0143e-04, -3.8865e-03]],\n",
            "\n",
            "         [[ 9.4187e-03,  5.0593e-03,  7.4841e-03],\n",
            "          [ 6.9562e-03,  7.8594e-03,  6.6831e-03],\n",
            "          [-1.1897e-02,  3.4482e-03,  1.1178e-02]],\n",
            "\n",
            "         [[-7.5184e-03, -5.2708e-05,  1.7704e-02],\n",
            "          [-1.9235e-02,  1.0645e-02, -4.2203e-03],\n",
            "          [-1.4986e-02,  2.4610e-03, -2.5846e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.1775e-03, -4.4026e-03, -3.8421e-04],\n",
            "          [-1.6223e-02,  1.6054e-02,  1.1512e-02],\n",
            "          [ 2.4116e-03,  1.7489e-02,  9.4777e-03]],\n",
            "\n",
            "         [[ 1.1931e-03,  1.9182e-02,  1.8205e-02],\n",
            "          [ 7.5272e-03,  1.3100e-02, -5.8502e-03],\n",
            "          [-4.8978e-03, -1.9927e-02,  1.3916e-02]],\n",
            "\n",
            "         [[-1.0377e-02, -8.5049e-03, -1.5593e-02],\n",
            "          [-6.7084e-03, -8.7571e-03, -1.1088e-02],\n",
            "          [-3.7716e-03,  2.0157e-02, -1.3546e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.6383e-03, -1.0276e-03,  7.0160e-03],\n",
            "          [ 1.4249e-02,  1.6307e-02, -9.8349e-03],\n",
            "          [ 1.9692e-02, -1.1640e-02,  8.1431e-03]],\n",
            "\n",
            "         [[-1.8042e-02,  2.6829e-03,  8.8602e-03],\n",
            "          [ 1.5200e-03,  1.4185e-02,  1.1359e-02],\n",
            "          [ 1.0115e-02, -1.4692e-02,  1.9716e-02]],\n",
            "\n",
            "         [[ 2.0116e-02,  7.3025e-03,  5.1462e-03],\n",
            "          [ 1.6981e-02,  1.0288e-02,  1.0066e-02],\n",
            "          [ 1.3000e-02,  1.4514e-02,  8.8225e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3504e-02, -1.2121e-02, -6.3274e-03],\n",
            "          [ 1.1139e-02,  1.5780e-02,  1.5624e-02],\n",
            "          [ 1.8414e-02,  1.2930e-02, -9.4245e-04]],\n",
            "\n",
            "         [[-1.9986e-02, -6.9526e-03, -2.0166e-02],\n",
            "          [-5.4638e-03, -1.9719e-02, -4.1999e-03],\n",
            "          [-1.8415e-02,  5.1233e-03, -1.6144e-02]],\n",
            "\n",
            "         [[ 1.4703e-02, -9.9969e-03,  1.0487e-02],\n",
            "          [-1.7191e-02, -1.6592e-02,  2.0483e-02],\n",
            "          [-1.4575e-02, -2.0845e-02, -8.6701e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4699e-02,  6.6238e-04, -9.6255e-03],\n",
            "          [-7.6415e-03, -1.0944e-02, -3.1564e-03],\n",
            "          [-4.4773e-03, -8.1606e-03, -8.5275e-03]],\n",
            "\n",
            "         [[-1.3642e-02,  1.9729e-02,  1.6005e-02],\n",
            "          [-2.0268e-02,  2.4180e-03, -1.8904e-02],\n",
            "          [ 1.3023e-02,  4.8960e-03, -8.1708e-03]],\n",
            "\n",
            "         [[ 7.8479e-05, -3.4129e-03, -6.7685e-03],\n",
            "          [-2.4858e-03,  5.1201e-03, -2.0013e-02],\n",
            "          [-1.0474e-02,  1.1243e-02,  1.4496e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "256\n",
            "Parameter containing:\n",
            "tensor([ 2.3622e-04, -3.2371e-03,  1.9430e-02,  1.6031e-02, -1.1261e-02,\n",
            "        -6.4563e-03, -5.5485e-03,  7.4376e-03, -3.4727e-04, -1.6812e-02,\n",
            "        -5.7590e-03, -1.7766e-02, -2.5424e-03,  9.2256e-04, -1.2589e-02,\n",
            "         8.7325e-03, -1.9962e-02, -1.5399e-02,  2.0612e-02,  4.3265e-03,\n",
            "        -8.8956e-03, -1.7613e-02,  2.2055e-03,  1.7264e-02,  6.3367e-03,\n",
            "         9.5549e-03,  1.9301e-02, -3.1331e-03, -2.0050e-02, -2.0746e-02,\n",
            "         1.9654e-02, -9.4898e-03, -1.6885e-02, -1.8850e-02, -4.9107e-03,\n",
            "         1.4752e-02, -1.3239e-02, -9.3452e-03,  1.0730e-02, -4.1144e-03,\n",
            "        -7.6189e-03, -1.7201e-02, -1.4513e-02, -4.2048e-03, -1.3547e-02,\n",
            "         1.5160e-02, -1.4984e-02,  1.5517e-02,  8.3808e-04,  1.0071e-02,\n",
            "         7.2616e-04, -7.5923e-03,  1.0672e-02,  3.9698e-03, -6.9450e-03,\n",
            "        -1.3500e-02,  3.4655e-03,  1.4761e-02, -6.9824e-03, -1.8177e-02,\n",
            "        -6.2113e-03, -2.7069e-04, -1.8335e-02, -1.0142e-02,  1.4992e-02,\n",
            "         6.3803e-03,  1.5952e-02,  2.0300e-03,  1.5518e-02, -3.5936e-03,\n",
            "        -6.8561e-03,  4.0382e-03, -1.2141e-03,  2.4914e-03,  1.9283e-02,\n",
            "        -1.4365e-02,  9.2328e-03,  9.5376e-03,  1.8021e-02, -6.3601e-03,\n",
            "        -6.3150e-03, -1.3695e-02,  1.3716e-02, -1.8798e-02, -4.6203e-03,\n",
            "         1.2820e-02, -1.2116e-02,  1.9594e-02,  4.7232e-03, -1.6716e-02,\n",
            "         6.6903e-03,  1.0340e-02,  1.0157e-02, -1.6794e-03, -4.3802e-03,\n",
            "         1.2821e-03, -1.6589e-02, -1.6251e-02,  2.9632e-03,  9.3483e-03,\n",
            "         3.9549e-03,  9.4836e-04, -1.1756e-02,  1.6354e-03, -1.0985e-03,\n",
            "        -9.1534e-03, -4.9419e-03, -6.4711e-03,  1.6261e-02, -1.2828e-02,\n",
            "         1.4687e-02,  6.1695e-03, -1.5624e-02,  1.3387e-02,  3.7096e-03,\n",
            "         1.6994e-02, -1.7040e-02, -5.7362e-03, -1.5386e-02,  1.5909e-02,\n",
            "        -2.8851e-04,  8.0052e-03, -8.2335e-03, -2.0356e-02,  1.3022e-02,\n",
            "         1.0160e-02,  1.4373e-02, -1.5462e-03, -7.7354e-03, -1.7306e-02,\n",
            "         2.5819e-03,  1.1170e-02,  1.2536e-03, -1.6608e-02, -6.8489e-03,\n",
            "        -8.3740e-03,  4.8434e-03, -1.2486e-02, -2.8265e-03, -1.9625e-03,\n",
            "         1.4616e-02, -1.8970e-02, -1.2275e-02,  8.6466e-04,  2.4361e-03,\n",
            "         9.0969e-03,  1.5596e-02, -9.0486e-03, -1.4368e-03, -3.2908e-03,\n",
            "         1.6721e-02,  6.4271e-04, -1.0247e-02, -1.3588e-02, -1.3686e-02,\n",
            "        -6.0006e-03, -2.0687e-03,  6.0733e-03,  3.5453e-03,  1.3455e-02,\n",
            "         8.1215e-03,  1.8447e-02,  1.8700e-02,  8.1106e-05,  1.5419e-02,\n",
            "        -9.5427e-03,  9.7700e-03,  7.7396e-03, -1.2345e-02, -1.2630e-02,\n",
            "         1.0310e-02, -7.2241e-03, -6.3224e-03, -5.3489e-03,  3.9241e-03,\n",
            "         1.8788e-02, -1.0188e-02, -7.4800e-03,  1.2496e-02,  1.9754e-02,\n",
            "        -7.2959e-03, -8.3222e-03, -7.3272e-03,  1.7022e-03, -2.2289e-03,\n",
            "         9.3461e-03, -3.3769e-03, -1.6602e-02, -1.0407e-02, -4.1687e-03,\n",
            "        -4.1850e-03,  1.0522e-02,  1.4318e-02, -2.8843e-04, -6.2623e-03,\n",
            "        -7.1013e-03,  1.1082e-02,  2.0699e-02, -1.1920e-02,  1.6611e-02,\n",
            "         1.6507e-02, -1.3323e-02, -8.4319e-03,  1.9668e-02,  1.2559e-02,\n",
            "        -7.6568e-03,  6.6251e-03,  1.9194e-02,  2.4370e-03, -8.2021e-03,\n",
            "        -9.9886e-03, -1.8830e-02, -6.5610e-03, -6.7885e-03, -7.5479e-03,\n",
            "         1.0828e-02, -6.8203e-03, -1.5506e-02,  1.1052e-02, -5.5494e-03,\n",
            "         1.4120e-02,  1.7050e-02,  1.3961e-02, -1.8261e-02, -5.3879e-03,\n",
            "        -6.3245e-03, -2.0781e-02,  2.9892e-03,  1.9552e-02, -1.4566e-02,\n",
            "         2.9627e-03,  1.4402e-02,  1.8776e-02,  7.9401e-03,  2.9848e-03,\n",
            "         1.2152e-03,  6.3248e-03, -1.0971e-02,  8.3349e-03, -1.3246e-02,\n",
            "        -1.8768e-02, -1.2889e-02,  7.8816e-03,  1.8000e-03, -4.1214e-03,\n",
            "         9.6760e-03,  6.8830e-03,  6.2341e-04, -4.6994e-03, -7.7021e-03,\n",
            "         7.8818e-03, -1.9803e-02,  7.6773e-03,  6.0208e-03,  3.6574e-03,\n",
            "         1.8338e-02], device='cuda:0', requires_grad=True)\n",
            "256\n",
            "Parameter containing:\n",
            "tensor([[ 0.0160,  0.0163,  0.0100,  ..., -0.0162,  0.0197, -0.0146],\n",
            "        [ 0.0194,  0.0164, -0.0045,  ..., -0.0177,  0.0152,  0.0105],\n",
            "        [ 0.0093,  0.0018, -0.0058,  ...,  0.0025, -0.0176, -0.0150],\n",
            "        ...,\n",
            "        [ 0.0053, -0.0044, -0.0183,  ...,  0.0069,  0.0082, -0.0200],\n",
            "        [-0.0108, -0.0057, -0.0141,  ...,  0.0198,  0.0101, -0.0119],\n",
            "        [-0.0185,  0.0095, -0.0204,  ..., -0.0048,  0.0051, -0.0003]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1024\n",
            "Parameter containing:\n",
            "tensor([ 0.0008, -0.0128, -0.0183,  ...,  0.0201, -0.0078, -0.0165],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "1024\n",
            "Parameter containing:\n",
            "tensor([[-0.0052,  0.0125, -0.0148,  ..., -0.0180, -0.0103,  0.0310],\n",
            "        [ 0.0183,  0.0190, -0.0143,  ...,  0.0301, -0.0130, -0.0258],\n",
            "        [-0.0116, -0.0047, -0.0066,  ..., -0.0300, -0.0029, -0.0261],\n",
            "        ...,\n",
            "        [-0.0121,  0.0018, -0.0082,  ..., -0.0151,  0.0134,  0.0221],\n",
            "        [-0.0167, -0.0243, -0.0004,  ...,  0.0201, -0.0126, -0.0011],\n",
            "        [ 0.0062,  0.0107,  0.0082,  ..., -0.0085, -0.0119,  0.0167]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "512\n",
            "Parameter containing:\n",
            "tensor([-1.3396e-02,  2.9108e-02,  7.9041e-03,  5.3460e-03, -1.6154e-02,\n",
            "         1.7228e-02,  3.1459e-03,  1.8237e-02,  2.1951e-02, -1.8062e-02,\n",
            "        -1.6245e-02,  1.8208e-02,  1.7086e-02,  2.5477e-02, -1.5599e-02,\n",
            "         1.5703e-02,  1.3046e-02,  2.9114e-02, -7.0722e-03,  7.3500e-03,\n",
            "         9.0019e-03, -4.0448e-03,  1.0807e-03, -3.0369e-02,  1.2167e-02,\n",
            "         2.0191e-02,  5.6727e-04, -7.8323e-03,  1.3071e-02,  1.9625e-03,\n",
            "        -1.3795e-02, -4.2083e-03, -3.3799e-04,  2.5219e-02, -2.0252e-02,\n",
            "         1.3137e-02, -3.9096e-03,  1.7566e-02, -1.6955e-02,  1.5412e-02,\n",
            "         7.4017e-03, -4.5357e-03, -1.6598e-02, -2.5778e-02,  1.3425e-02,\n",
            "         1.0835e-02,  1.8366e-02, -2.6696e-02, -2.4903e-02,  1.6331e-02,\n",
            "         1.0068e-02,  4.2751e-03,  1.8339e-02,  8.8175e-03, -1.1839e-02,\n",
            "         1.6198e-02,  2.0578e-03, -2.3640e-03,  1.9962e-02, -2.1666e-02,\n",
            "         5.2373e-03,  9.5222e-03,  1.6449e-02, -2.3787e-02, -2.4320e-02,\n",
            "        -9.0569e-03,  6.1582e-04,  1.4997e-02,  1.0119e-02,  2.8302e-02,\n",
            "         3.4139e-04,  2.5786e-02,  6.1736e-03,  3.0796e-02,  2.3516e-02,\n",
            "        -6.6379e-03, -1.6209e-02,  2.5044e-02, -2.2675e-02,  2.7822e-02,\n",
            "         1.6917e-02, -1.3031e-03, -1.0598e-03,  2.5719e-02,  1.9977e-02,\n",
            "         2.9941e-02, -2.1306e-02, -4.9755e-03, -1.1019e-02,  2.2878e-02,\n",
            "         1.9743e-02, -3.5853e-03,  2.4386e-02,  7.0506e-03,  1.7747e-02,\n",
            "        -3.0307e-02, -1.7793e-02, -2.1173e-02, -2.8187e-02, -2.0056e-02,\n",
            "         2.4937e-03, -2.4994e-02,  3.0404e-02,  1.9193e-02,  2.6038e-02,\n",
            "         5.4441e-03,  2.4078e-02, -2.1493e-02, -1.8135e-03,  4.5914e-03,\n",
            "         2.3223e-02,  2.9664e-02, -2.7509e-02,  2.2453e-02, -1.9175e-02,\n",
            "        -2.4256e-02, -2.3002e-02, -2.2479e-02, -2.9301e-02, -6.0627e-03,\n",
            "         1.3556e-03, -1.6436e-02,  7.8891e-03,  4.7219e-03,  2.0985e-02,\n",
            "        -2.4603e-02, -1.4153e-02, -3.0501e-02, -2.1205e-03,  2.2939e-02,\n",
            "        -1.1215e-02, -1.5698e-02,  1.8985e-04,  2.2723e-02,  3.9540e-03,\n",
            "         2.0526e-02, -1.6402e-02,  6.2038e-03,  2.8973e-02, -2.8181e-02,\n",
            "         1.7074e-02, -4.6506e-03, -1.2935e-02,  1.7887e-02,  3.1550e-03,\n",
            "         1.4587e-02, -2.2102e-02, -1.0744e-02,  1.0119e-02, -3.5199e-03,\n",
            "         1.3922e-02, -2.4881e-02,  7.6747e-03,  1.8929e-02,  2.7443e-02,\n",
            "         2.3140e-02, -1.1245e-02,  4.1030e-03, -1.7503e-03, -1.4594e-02,\n",
            "        -3.7656e-03,  2.3617e-03, -2.6393e-02,  6.9925e-03,  7.1986e-03,\n",
            "         2.7257e-02, -1.2174e-02, -1.9750e-02, -2.0727e-03,  2.0424e-02,\n",
            "        -2.3064e-02,  2.8429e-02,  5.0091e-03, -1.2527e-02, -2.0743e-02,\n",
            "        -1.5061e-02, -1.1017e-02,  1.3505e-02,  6.9326e-03,  1.8696e-02,\n",
            "        -2.5106e-02,  2.9551e-02, -2.2268e-03,  2.3601e-02,  1.8971e-02,\n",
            "        -1.7697e-02,  9.6551e-03,  1.4406e-02, -1.6318e-02,  6.9678e-03,\n",
            "        -9.3698e-03, -2.6285e-02,  3.9211e-03, -2.0667e-04, -1.4539e-02,\n",
            "        -3.3271e-03,  2.4554e-02, -2.4539e-02, -1.7981e-02,  2.5357e-02,\n",
            "        -2.6328e-03, -1.4440e-02, -2.8687e-02,  1.5890e-02,  2.7261e-02,\n",
            "        -4.0506e-03,  1.8170e-02, -2.7774e-02, -7.2221e-03, -1.4679e-02,\n",
            "        -2.6746e-02,  2.3723e-02,  5.7342e-03,  2.4009e-02, -1.9455e-02,\n",
            "         4.8905e-03,  2.7308e-02, -2.8567e-03, -7.2036e-03,  1.4106e-02,\n",
            "        -2.0897e-02,  2.6534e-02, -2.0751e-02, -2.8273e-02,  2.5048e-02,\n",
            "        -2.4819e-02, -1.7304e-02,  9.7037e-03,  9.6159e-03, -2.6488e-02,\n",
            "         9.9195e-03, -2.6702e-02,  1.9000e-02,  2.1094e-02,  2.6175e-03,\n",
            "        -8.7682e-04,  2.6418e-02,  2.5424e-02,  2.3137e-02, -5.2814e-03,\n",
            "         1.6520e-03, -9.8736e-04,  1.2271e-02, -1.0667e-02,  1.4204e-02,\n",
            "         6.4846e-03, -1.7355e-02, -4.7556e-03,  3.0329e-02, -6.3786e-03,\n",
            "         1.6549e-02,  2.3806e-02, -5.6250e-03, -2.7000e-02,  8.5562e-03,\n",
            "         2.3501e-02,  8.1568e-03, -2.1150e-02, -1.0567e-02,  2.6265e-02,\n",
            "         1.0941e-02,  9.7879e-03,  1.6664e-02,  9.3735e-03,  2.6336e-02,\n",
            "        -2.3939e-02, -3.0986e-02, -1.0364e-02, -3.9445e-03,  8.2743e-03,\n",
            "         1.8787e-02,  1.0503e-02,  2.0770e-02, -1.1711e-02, -1.3308e-02,\n",
            "        -1.7399e-04, -2.2996e-02,  2.3199e-02, -3.0267e-02, -2.3686e-02,\n",
            "         1.2104e-02,  8.8349e-03, -6.7367e-03,  8.6195e-03,  1.5220e-03,\n",
            "        -2.0493e-02,  3.0169e-03, -2.0294e-02,  1.1623e-02, -2.0342e-03,\n",
            "         1.8952e-02,  1.0900e-02, -1.1733e-02, -1.6968e-02, -1.8578e-02,\n",
            "         1.6197e-02, -1.6908e-02, -8.1558e-03, -1.2107e-02,  3.0252e-02,\n",
            "         3.0270e-03,  2.3671e-02,  2.0008e-02,  2.9186e-02,  2.0891e-02,\n",
            "        -4.0059e-03, -1.2493e-02,  1.5124e-02,  2.1191e-02,  6.0120e-03,\n",
            "         1.5249e-02,  1.3229e-02,  2.8263e-02, -4.7364e-03, -1.9383e-03,\n",
            "        -1.0012e-02,  5.6103e-03,  1.3893e-02, -2.8261e-02,  2.5719e-02,\n",
            "        -1.5779e-02,  4.8287e-03,  1.8403e-03, -1.5967e-02,  1.1689e-02,\n",
            "        -2.3067e-02,  9.8734e-03,  2.3468e-02, -2.1632e-02, -3.5903e-03,\n",
            "        -4.8003e-03,  2.8918e-02, -1.5045e-02,  1.7906e-02,  8.3320e-03,\n",
            "        -2.6192e-02,  6.8869e-03,  1.9970e-03, -1.1622e-02, -2.0024e-02,\n",
            "        -2.4119e-02,  2.5585e-02,  2.0637e-02,  1.3732e-02,  8.9988e-03,\n",
            "         8.9052e-03, -2.5518e-02,  2.0382e-02,  1.9231e-02,  2.5491e-02,\n",
            "         8.6424e-03, -3.0128e-02,  9.7516e-04, -1.5840e-02, -2.9551e-02,\n",
            "         7.8395e-03,  9.4539e-03,  2.1309e-02,  1.8889e-02, -5.7545e-03,\n",
            "        -2.6937e-02, -7.2181e-05,  6.4701e-03,  2.5079e-02,  2.1383e-03,\n",
            "         2.1269e-02,  1.3235e-02,  1.4231e-02, -2.1286e-02, -2.9282e-02,\n",
            "        -2.7982e-02, -3.2792e-03,  1.8104e-02, -1.3614e-02,  1.5034e-02,\n",
            "        -2.6218e-02, -2.3765e-02,  1.1345e-02,  1.4880e-02,  2.6324e-03,\n",
            "         1.7211e-02, -1.5113e-02,  2.5687e-02,  9.9617e-03, -1.1224e-02,\n",
            "         3.0085e-02,  2.2464e-02, -3.4054e-03, -1.8254e-02, -4.3870e-03,\n",
            "         2.4446e-02, -1.6840e-02,  3.0112e-02, -2.2982e-02, -9.5960e-03,\n",
            "         4.3813e-03,  3.1182e-02, -2.0205e-02, -8.0855e-03, -6.6770e-03,\n",
            "         3.1856e-03, -2.9720e-02,  2.1997e-02,  3.1267e-02,  3.6288e-03,\n",
            "        -1.7234e-02, -2.9695e-02, -2.2582e-04, -2.9664e-02, -2.8364e-02,\n",
            "         4.8971e-03,  2.6360e-02,  2.9040e-02, -6.7534e-03, -3.9032e-03,\n",
            "        -2.3723e-02, -1.5088e-02,  2.3254e-03,  9.9147e-03, -2.5824e-02,\n",
            "         2.9109e-02,  2.7502e-02, -9.9259e-03,  1.0764e-02,  9.1047e-04,\n",
            "        -5.9920e-03, -1.8664e-02, -5.9798e-03,  9.6140e-03,  1.1869e-02,\n",
            "         2.9349e-02, -2.6410e-02, -3.6335e-03,  5.9342e-04,  1.5136e-02,\n",
            "         2.2979e-02, -4.0917e-03, -1.1222e-02, -6.6762e-03, -2.4394e-02,\n",
            "         2.3804e-02, -1.1718e-02, -2.7112e-03,  9.6606e-03,  5.8612e-04,\n",
            "         6.7551e-03, -8.6421e-03,  4.0747e-03,  1.7323e-03, -2.9033e-02,\n",
            "        -2.1086e-02, -1.2099e-02, -2.7888e-03, -1.0233e-02,  2.3304e-02,\n",
            "        -2.1667e-02,  1.5260e-02, -2.1649e-02, -1.3686e-02,  1.4066e-02,\n",
            "        -9.9309e-03,  3.3562e-03,  2.6329e-02, -2.7005e-02,  3.3267e-03,\n",
            "        -1.8195e-02, -6.7703e-03, -7.6356e-03,  3.0937e-02,  1.6082e-03,\n",
            "        -2.5947e-02, -2.8573e-02,  2.9176e-02,  2.3969e-02,  1.8073e-02,\n",
            "         2.1249e-02,  1.8478e-02,  1.9353e-02,  4.2513e-03,  3.9805e-03,\n",
            "        -2.0584e-02,  2.7436e-02, -2.6454e-02,  2.1679e-02, -6.0362e-03,\n",
            "         2.8329e-02,  1.8768e-02,  3.0995e-03,  1.5297e-02,  9.2830e-03,\n",
            "        -2.3907e-02,  1.6626e-02, -2.4898e-02, -2.0812e-02,  2.7783e-02,\n",
            "         1.2276e-02,  2.3706e-02, -1.4268e-02, -1.8963e-02, -2.8684e-02,\n",
            "        -1.6969e-02, -1.9062e-02, -1.4282e-02, -1.4596e-02,  2.3310e-02,\n",
            "        -9.7599e-03,  1.4236e-02,  7.2057e-04,  1.7778e-02,  1.8825e-02,\n",
            "        -7.6883e-03, -1.0360e-02], device='cuda:0', requires_grad=True)\n",
            "512\n",
            "Parameter containing:\n",
            "tensor([[-0.0150, -0.0217, -0.0319,  ...,  0.0163,  0.0086,  0.0211],\n",
            "        [ 0.0054,  0.0424, -0.0423,  ..., -0.0023,  0.0105,  0.0105],\n",
            "        [-0.0353,  0.0106,  0.0117,  ...,  0.0305, -0.0036, -0.0339],\n",
            "        ...,\n",
            "        [-0.0130,  0.0098,  0.0095,  ..., -0.0064, -0.0323, -0.0362],\n",
            "        [-0.0504,  0.0295, -0.0100,  ..., -0.0260,  0.0170, -0.0247],\n",
            "        [-0.0285,  0.0357,  0.0103,  ...,  0.0367,  0.0372, -0.0221]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "10\n",
            "Parameter containing:\n",
            "tensor([-0.0046, -0.0346, -0.0367, -0.0343,  0.0080, -0.0314, -0.0257, -0.0133,\n",
            "         0.0133, -0.0360], device='cuda:0', requires_grad=True)\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrktgHPr6Ten"
      },
      "source": [
        "## 目標malware執行檔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "M-yZZtP0AHNG",
        "outputId": "23348180-3635-4ab4-c756-a1745a031a98"
      },
      "source": [
        "malware = pd.read_csv(\"malFloat.csv\")\n",
        "malware = malware.drop(columns = 'Unnamed: 0')\n",
        "malware.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.012534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.007813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.007813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.007812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.031250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0\n",
              "0  0.012534\n",
              "1  0.007813\n",
              "2  0.007813\n",
              "3  0.007812\n",
              "4  0.031250"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMFbqZZyF1Mq",
        "outputId": "87c78c3c-f42d-4bd9-c68a-e3c034ab3447"
      },
      "source": [
        "malFloat = malware['0'].tolist()\n",
        "print(len(malFloat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDflbXkVKro2",
        "outputId": "3fdb413f-9674-4eab-f488-2c582cf2ae50"
      },
      "source": [
        "weight_matrix = torch.tensor([malFloat[:512]], dtype=torch.float32)\n",
        "print(weight_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0125, 0.0078, 0.0078, 0.0078, 0.0312, 0.0079, 0.0078, 0.0078, 0.0117,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0283, 0.0078, 0.0227, 0.0220, 0.0099, 0.0125, 0.0130,\n",
            "         0.0148, 0.0148, 0.0148, 0.0098, 0.0146, 0.0149, 0.0140, 0.0150, 0.0142,\n",
            "         0.0120, 0.0098, 0.0139, 0.0086, 0.0100, 0.0078, 0.0078, 0.0114, 0.0282,\n",
            "         0.0101, 0.0087, 0.0134, 0.0149, 0.0101, 0.0087, 0.0242, 0.0242, 0.0101,\n",
            "         0.0200, 0.0134, 0.0309, 0.0101, 0.0087, 0.0100, 0.0276, 0.0101, 0.0087,\n",
            "         0.0134, 0.0309, 0.0101, 0.0087, 0.0242, 0.0281, 0.0101, 0.0180, 0.0134,\n",
            "         0.0128, 0.0142, 0.0087, 0.0078, 0.0078, 0.0078, 0.0078, 0.0164, 0.0155,\n",
            "         0.0134, 0.0078, 0.0078, 0.0293, 0.0078, 0.0084, 0.0288, 0.0079, 0.0078,\n",
            "         0.0078, 0.0111, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0079, 0.0078, 0.0078, 0.0078, 0.0078, 0.0162,\n",
            "         0.0080, 0.0157, 0.0088, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0079,\n",
            "         0.0079, 0.0078, 0.0079, 0.0079, 0.0078, 0.0176, 0.0095, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0080, 0.0095, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0225, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0140, 0.0078, 0.0292, 0.0078, 0.0078,\n",
            "         0.0288, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0148,\n",
            "         0.0149, 0.0079, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0106, 0.0149, 0.0078, 0.0117, 0.0078, 0.0078,\n",
            "         0.0078, 0.0103, 0.0078, 0.0078, 0.0078, 0.0078, 0.0117, 0.0235, 0.0139,\n",
            "         0.0137, 0.0095, 0.0078, 0.0079, 0.0085, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0148, 0.0139, 0.0079, 0.0078, 0.0195, 0.0078,\n",
            "         0.0078, 0.0079, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0106, 0.0144,\n",
            "         0.0078, 0.0080, 0.0078, 0.0078, 0.0078, 0.0188, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0117, 0.0118, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
            "         0.0150, 0.0171, 0.0168, 0.0125, 0.0093, 0.0124, 0.0159, 0.0083, 0.0122,\n",
            "         0.0204, 0.0078, 0.0240, 0.0161, 0.0083, 0.0125, 0.0227, 0.0078, 0.0172,\n",
            "         0.0120, 0.0312, 0.0125, 0.0122, 0.0100, 0.0202, 0.0079, 0.0089, 0.0125,\n",
            "         0.0100, 0.0172, 0.0122, 0.0273, 0.0079, 0.0086, 0.0226, 0.0078, 0.0091,\n",
            "         0.0078, 0.0239, 0.0163, 0.0145, 0.0269, 0.0129, 0.0168, 0.0125, 0.0098,\n",
            "         0.0288, 0.0078, 0.0170, 0.0109, 0.0122, 0.0122, 0.0100, 0.0078, 0.0171,\n",
            "         0.0078, 0.0078, 0.0119, 0.0172, 0.0122, 0.0100, 0.0168, 0.0098, 0.0137,\n",
            "         0.0284, 0.0078, 0.0172, 0.0108, 0.0122, 0.0172, 0.0085, 0.0284, 0.0078,\n",
            "         0.0079, 0.0079, 0.0096, 0.0079, 0.0250, 0.0168, 0.0083, 0.0149, 0.0131,\n",
            "         0.0288, 0.0082, 0.0079, 0.0171, 0.0171, 0.0268, 0.0246, 0.0119, 0.0233,\n",
            "         0.0171, 0.0162, 0.0078, 0.0244, 0.0235, 0.0170, 0.0118, 0.0122, 0.0118,\n",
            "         0.0123, 0.0265, 0.0236, 0.0246, 0.0120, 0.0288, 0.0170, 0.0112, 0.0238,\n",
            "         0.0134, 0.0122, 0.0098, 0.0249, 0.0134, 0.0122, 0.0100, 0.0168, 0.0098,\n",
            "         0.0130, 0.0118, 0.0131, 0.0288, 0.0288, 0.0171, 0.0172, 0.0085, 0.0122,\n",
            "         0.0147, 0.0078, 0.0125, 0.0110, 0.0168, 0.0284, 0.0078, 0.0171, 0.0235,\n",
            "         0.0091, 0.0079, 0.0182, 0.0255, 0.0312, 0.0125, 0.0172, 0.0079, 0.0283,\n",
            "         0.0078, 0.0170, 0.0284, 0.0078, 0.0170, 0.0125, 0.0118, 0.0309, 0.0312,\n",
            "         0.0118, 0.0308, 0.0235, 0.0284, 0.0078, 0.0170, 0.0121, 0.0100, 0.0081,\n",
            "         0.0078, 0.0263, 0.0251, 0.0243, 0.0093, 0.0123, 0.0122, 0.0284]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtUpFwP11N7F"
      },
      "source": [
        "## 將parameter換成malware"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upm5Tl0-1TLe"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(AlexNet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(1, 32, kernel_size=3, padding=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((3, 3))\n",
        "    nn.Linear(512, num_classes).weight = nn.Parameter(weight_matrix)\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(256 * 3 * 3, 1024),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Dropout(),\n",
        "      nn.Linear(1024, 512),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(512, num_classes),\n",
        "      \n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa9k3L6myfUZ",
        "outputId": "88842e0a-6f2e-496a-f6bc-3ba5d3d2060b"
      },
      "source": [
        "nn.Linear(1024, 512).weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0123,  0.0263, -0.0226,  ...,  0.0269,  0.0063,  0.0175],\n",
              "        [-0.0049, -0.0250, -0.0092,  ...,  0.0218,  0.0187,  0.0222],\n",
              "        [ 0.0065,  0.0130,  0.0200,  ...,  0.0055, -0.0153, -0.0257],\n",
              "        ...,\n",
              "        [-0.0062, -0.0181, -0.0270,  ...,  0.0164,  0.0096,  0.0227],\n",
              "        [-0.0237, -0.0091, -0.0051,  ...,  0.0273, -0.0119, -0.0161],\n",
              "        [-0.0202,  0.0117, -0.0012,  ...,  0.0069,  0.0117, -0.0129]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOKxgbG5ibB0",
        "outputId": "fc426997-5d8f-435f-affa-30d3c3f575fe"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AlexNet(num_classes=10).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (x,y) in enumerate(dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if batch%100==0:\n",
        "    loss, current = loss.item(), batch * len(x)\n",
        "    print(f\"loss: {loss: >7f}[{curret: >5d}/{size: >5d}]\")\n",
        "\n",
        "def _test(dataloader, model):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0,0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      pred = model(x)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "  test_loss /= size\n",
        "  correct /= size\n",
        "  print(f\"Test error: \\n Acc: {correct}, Avg Loss:{test_loss}\")\n",
        "\n",
        "epoch = 5\n",
        "for t in range(epoch):\n",
        "  print(f\"Epoch {t+1}\\n-----------------\")\n",
        "  train(train_loader, model, loss_fn, optimizer)\n",
        "  _test(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-----------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test error: \n",
            " Acc: 0.71135, Avg Loss:0.007520571096738179\n",
            "Epoch 2\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.81275, Avg Loss:0.005302051740388076\n",
            "Epoch 3\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8392333333333334, Avg Loss:0.004604297307133675\n",
            "Epoch 4\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8541833333333333, Avg Loss:0.004170298538108666\n",
            "Epoch 5\n",
            "-----------------\n",
            "Test error: \n",
            " Acc: 0.8631166666666666, Avg Loss:0.0038730158237119516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enb9mbsKwh7I",
        "outputId": "a6ef0f15-a0d4-4989-8b2c-ed187abf5865"
      },
      "source": [
        "nn.Linear(1024, 512).weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0111,  0.0274, -0.0309,  ..., -0.0149,  0.0040,  0.0125],\n",
              "        [ 0.0309, -0.0258,  0.0127,  ...,  0.0188,  0.0123, -0.0168],\n",
              "        [ 0.0164,  0.0029,  0.0057,  ..., -0.0035,  0.0170, -0.0106],\n",
              "        ...,\n",
              "        [ 0.0133,  0.0293,  0.0298,  ...,  0.0230,  0.0037, -0.0116],\n",
              "        [-0.0139,  0.0048,  0.0130,  ..., -0.0139, -0.0203, -0.0111],\n",
              "        [-0.0306, -0.0030,  0.0108,  ...,  0.0102, -0.0106,  0.0099]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zAPg4C1xas0",
        "outputId": "67a3138f-3de6-487d-e415-7e114fcbaca9"
      },
      "source": [
        "weight_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0125, 0.0078, 0.0078, 0.0078, 0.0312, 0.0079, 0.0078, 0.0078, 0.0117,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0283, 0.0078, 0.0227, 0.0220, 0.0099, 0.0125, 0.0130,\n",
              "         0.0148, 0.0148, 0.0148, 0.0098, 0.0146, 0.0149, 0.0140, 0.0150, 0.0142,\n",
              "         0.0120, 0.0098, 0.0139, 0.0086, 0.0100, 0.0078, 0.0078, 0.0114, 0.0282,\n",
              "         0.0101, 0.0087, 0.0134, 0.0149, 0.0101, 0.0087, 0.0242, 0.0242, 0.0101,\n",
              "         0.0200, 0.0134, 0.0309, 0.0101, 0.0087, 0.0100, 0.0276, 0.0101, 0.0087,\n",
              "         0.0134, 0.0309, 0.0101, 0.0087, 0.0242, 0.0281, 0.0101, 0.0180, 0.0134,\n",
              "         0.0128, 0.0142, 0.0087, 0.0078, 0.0078, 0.0078, 0.0078, 0.0164, 0.0155,\n",
              "         0.0134, 0.0078, 0.0078, 0.0293, 0.0078, 0.0084, 0.0288, 0.0079, 0.0078,\n",
              "         0.0078, 0.0111, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0079, 0.0078, 0.0078, 0.0078, 0.0078, 0.0162,\n",
              "         0.0080, 0.0157, 0.0088, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0079,\n",
              "         0.0079, 0.0078, 0.0079, 0.0079, 0.0078, 0.0176, 0.0095, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0080, 0.0095, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0225, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0140, 0.0078, 0.0292, 0.0078, 0.0078,\n",
              "         0.0288, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0148,\n",
              "         0.0149, 0.0079, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0106, 0.0149, 0.0078, 0.0117, 0.0078, 0.0078,\n",
              "         0.0078, 0.0103, 0.0078, 0.0078, 0.0078, 0.0078, 0.0117, 0.0235, 0.0139,\n",
              "         0.0137, 0.0095, 0.0078, 0.0079, 0.0085, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0148, 0.0139, 0.0079, 0.0078, 0.0195, 0.0078,\n",
              "         0.0078, 0.0079, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0106, 0.0144,\n",
              "         0.0078, 0.0080, 0.0078, 0.0078, 0.0078, 0.0188, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0117, 0.0118, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "         0.0150, 0.0171, 0.0168, 0.0125, 0.0093, 0.0124, 0.0159, 0.0083, 0.0122,\n",
              "         0.0204, 0.0078, 0.0240, 0.0161, 0.0083, 0.0125, 0.0227, 0.0078, 0.0172,\n",
              "         0.0120, 0.0312, 0.0125, 0.0122, 0.0100, 0.0202, 0.0079, 0.0089, 0.0125,\n",
              "         0.0100, 0.0172, 0.0122, 0.0273, 0.0079, 0.0086, 0.0226, 0.0078, 0.0091,\n",
              "         0.0078, 0.0239, 0.0163, 0.0145, 0.0269, 0.0129, 0.0168, 0.0125, 0.0098,\n",
              "         0.0288, 0.0078, 0.0170, 0.0109, 0.0122, 0.0122, 0.0100, 0.0078, 0.0171,\n",
              "         0.0078, 0.0078, 0.0119, 0.0172, 0.0122, 0.0100, 0.0168, 0.0098, 0.0137,\n",
              "         0.0284, 0.0078, 0.0172, 0.0108, 0.0122, 0.0172, 0.0085, 0.0284, 0.0078,\n",
              "         0.0079, 0.0079, 0.0096, 0.0079, 0.0250, 0.0168, 0.0083, 0.0149, 0.0131,\n",
              "         0.0288, 0.0082, 0.0079, 0.0171, 0.0171, 0.0268, 0.0246, 0.0119, 0.0233,\n",
              "         0.0171, 0.0162, 0.0078, 0.0244, 0.0235, 0.0170, 0.0118, 0.0122, 0.0118,\n",
              "         0.0123, 0.0265, 0.0236, 0.0246, 0.0120, 0.0288, 0.0170, 0.0112, 0.0238,\n",
              "         0.0134, 0.0122, 0.0098, 0.0249, 0.0134, 0.0122, 0.0100, 0.0168, 0.0098,\n",
              "         0.0130, 0.0118, 0.0131, 0.0288, 0.0288, 0.0171, 0.0172, 0.0085, 0.0122,\n",
              "         0.0147, 0.0078, 0.0125, 0.0110, 0.0168, 0.0284, 0.0078, 0.0171, 0.0235,\n",
              "         0.0091, 0.0079, 0.0182, 0.0255, 0.0312, 0.0125, 0.0172, 0.0079, 0.0283,\n",
              "         0.0078, 0.0170, 0.0284, 0.0078, 0.0170, 0.0125, 0.0118, 0.0309, 0.0312,\n",
              "         0.0118, 0.0308, 0.0235, 0.0284, 0.0078, 0.0170, 0.0121, 0.0100, 0.0081,\n",
              "         0.0078, 0.0263, 0.0251, 0.0243, 0.0093, 0.0123, 0.0122, 0.0284]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCkzpMerxL4S",
        "outputId": "ae503ed0-5ca3-4e73-e3d3-8afe2aecd212"
      },
      "source": [
        "nn.Linear(1024, 512).weight = nn.Parameter(weight_matrix)\n",
        "nn.Linear(1024, 512).weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0093, -0.0292,  0.0052,  ..., -0.0253,  0.0208, -0.0310],\n",
              "        [-0.0028, -0.0043, -0.0052,  ..., -0.0280,  0.0201,  0.0175],\n",
              "        [-0.0264,  0.0298,  0.0048,  ..., -0.0017, -0.0052, -0.0233],\n",
              "        ...,\n",
              "        [ 0.0006,  0.0117,  0.0071,  ..., -0.0151, -0.0241, -0.0125],\n",
              "        [ 0.0085, -0.0116, -0.0174,  ...,  0.0036, -0.0292, -0.0217],\n",
              "        [-0.0285, -0.0301,  0.0296,  ..., -0.0263, -0.0231, -0.0204]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yzj2g-w5vjh"
      },
      "source": [
        "**Reference**\\\n",
        "https://blog.csdn.net/qq_45465526/article/details/115918610"
      ]
    }
  ]
}